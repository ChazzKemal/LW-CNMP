{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from models import LocallyWeightedCNP,CNP,LW_VQCNP,LW_VQCNP_2\n",
    "import data\n",
    "from ipywidgets import interact, FloatSlider\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save_keypoints(pkl_folder_path, output_folder_path):\n",
    "    pkl_folder = Path(pkl_folder_path)\n",
    "    output_folder = Path(output_folder_path)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)  # Create the output folder if it doesn't exist\n",
    "\n",
    "    # Initialize an empty list to hold all the curves\n",
    "    data = []\n",
    "\n",
    "    for pkl_file in pkl_folder.glob('*.pkl'):\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            # Load the demonstration data from the file\n",
    "            demonstration = pickle.load(f)\n",
    "            \n",
    "            # Extract the optimized keypoints data\n",
    "            keypoints3d_optim = demonstration['keypoints3d_optim']\n",
    "            \n",
    "            \n",
    "            # Ensure the number of timesteps is 524\n",
    "            num_timesteps = keypoints3d_optim.shape[0]\n",
    "            if num_timesteps > 200:\n",
    "                keypoints3d_optim = keypoints3d_optim[:200]\n",
    "            # Select the x and y coordinates\n",
    "            keypoints_xy = keypoints3d_optim[:, :, :3]  # Assuming the shape is (N, 17, 3)\n",
    "\n",
    "            # Flatten the last two dimensions\n",
    "            keypoints_flat = keypoints_xy.reshape(keypoints_xy.shape[0], -1)  # New shape (N, 34)\n",
    "            \n",
    "            # Generate the timesteps tensor\n",
    "            timesteps = torch.linspace(0, 1, keypoints_flat.shape[0]).unsqueeze(1)\n",
    "\n",
    "            # Concatenate timesteps with the flattened keypoints\n",
    "            curve = torch.cat([timesteps, torch.tensor(keypoints_flat, dtype=torch.float32)], dim=1)\n",
    "\n",
    "            # Append the curve to the data list\n",
    "            data.append(curve.numpy())  # Convert to numpy for universal compatibility\n",
    "            \n",
    "            # Save the individual processed curve to a new file (optional)\n",
    "            output_file_path = output_folder / f\"{pkl_file.stem}_processed.pkl\"\n",
    "            with open(output_file_path, 'wb') as out_f:\n",
    "                pickle.dump(curve.numpy(), out_f)\n",
    "\n",
    "    # Save the entire data list to a single file\n",
    "    data_pkl_path = output_folder / 'data_3d.pkl'\n",
    "    with open(data_pkl_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "\n",
    "# Usage:\n",
    "pkl_folder_path = '/home/colors/aist_lw/aistplusplus_api-main/3d_keypoint_pkls'\n",
    "output_folder_path = '/home/colors/aist_lw/aistplusplus_api-main/3d_keypoint_pkls/processed_3d'\n",
    "preprocess_and_save_keypoints(pkl_folder_path, output_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_and_save_keypoints(pkl_folder_path, output_folder_path):\n",
    "    pkl_folder = Path(pkl_folder_path)\n",
    "    output_folder = Path(output_folder_path)\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to hold all the curves\n",
    "    data = []\n",
    "\n",
    "    # Find the maximum absolute value for normalization\n",
    "    max_val = float('-inf')\n",
    "    for pkl_file in pkl_folder.glob('*.pkl'):\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            demonstration = pickle.load(f)\n",
    "            keypoints3d_optim = demonstration['keypoints3d_optim']\n",
    "            if keypoints3d_optim.shape[0] > 200:\n",
    "                keypoints3d_optim = keypoints3d_optim[:200]\n",
    "            keypoints_xy = keypoints3d_optim[:, :, :2]\n",
    "            current_max = np.abs(keypoints_xy).max()\n",
    "            max_val = max(max_val, current_max)\n",
    "\n",
    "    # Process the keypoints and normalize them\n",
    "    for pkl_file in pkl_folder.glob('*.pkl'):\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            demonstration = pickle.load(f)\n",
    "            keypoints3d_optim = demonstration['keypoints3d_optim']\n",
    "            if keypoints3d_optim.shape[0] > 200:\n",
    "                keypoints3d_optim = keypoints3d_optim[:200]\n",
    "            keypoints_xy = keypoints3d_optim[:, :, :2]\n",
    "\n",
    "            # Normalize the keypoints to be between -1 and 1\n",
    "            keypoints_xy /= max_val\n",
    "\n",
    "            keypoints_flat = keypoints_xy.reshape(keypoints_xy.shape[0], -1)\n",
    "            timesteps = torch.linspace(0, 1, keypoints_flat.shape[0]).unsqueeze(1)\n",
    "            curve = torch.cat([timesteps, torch.tensor(keypoints_flat, dtype=torch.float32)], dim=1)\n",
    "            data.append(curve.numpy())\n",
    "\n",
    "            output_file_path = output_folder / f\"{pkl_file.stem}_processed.pkl\"\n",
    "            with open(output_file_path, 'wb') as out_f:\n",
    "                pickle.dump(curve.numpy(), out_f)\n",
    "\n",
    "    data_pkl_path = output_folder / 'data.pkl'\n",
    "    with open(data_pkl_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "# Usage example\n",
    "preprocess_and_save_keypoints(pkl_folder_path, output_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 35])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62097/275004954.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  self.data= torch.tensor(self.data)\n"
     ]
    }
   ],
   "source": [
    "# with open(output_folder_path+'/data.pkl', 'rb') as f:\n",
    "#     data_dance = pickle.load(f)\n",
    "# N = len(data_dance)\n",
    "# N\n",
    "class Dance_Dataset(data.CNPDemonstrationDataset):\n",
    "    def __init__(self, data_pkl_path):\n",
    "        # Load the data from the .pkl file\n",
    "        with open(data_pkl_path, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "            self.data= torch.tensor(self.data)\n",
    "        self.N = len(self.data)\n",
    "data_pkl_path='/home/colors/aist_lw/aistplusplus_api-main/3d_keypoint_pkls/processed/data.pkl'\n",
    "dset = Dance_Dataset(data_pkl_path)\n",
    "print(dset.data[0].shape)\n",
    "dset.N=2\n",
    "dset.data=dset.data[[0,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bad=CNP((1, 34), hidden_size=256, num_hidden_layers=6, min_std=0.0001).to(device)\n",
    "learning_rate = 1e-4\n",
    "optimizer_bad = torch.optim.Adam(model_bad.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bad.load_state_dict(torch.load(\"/home/colors/aist_lw/aistplusplus_api-main/cnp_dance_0_6_bad.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LocallyWeightedCNP((1, 34), hidden_size=256, num_hidden_layers=6, min_std=0.0001).to(device)\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/home/colors/aist_lw/aistplusplus_api-main/cnp_dance_0_6.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation=torch.stack((dset.data[1][0],dset.data[0][199])).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 34])\n",
      "15.46585750579834 50.13927459716797\n"
     ]
    }
   ],
   "source": [
    "#observation= torch.tensor([dset.data[0][0]]).unsqueeze(0)\n",
    "target = torch.linspace(0.0, 1.0, 200).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "#observation=dset.data[0][0].unsqueeze(0).to(device)\n",
    "observation=torch.stack((dset.data[1][0],dset.data[0][199])).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    #model.weight_std = torch.tensor(0.5)\n",
    "    mean, std = model(observation, target, locally_weighted=True,aggregation_std=0.2)\n",
    "    mean2, std2 = model_bad(observation, target)\n",
    "    #mean, std = model(observation, target, locally_weighted=True,aggregation_std=0.5,primitive_index=torch.tensor([0,0]).to(device))\n",
    "    #mean, std = model(observation, target)\n",
    "mean.squeeze_(0)\n",
    "std.squeeze_(0)\n",
    "mean2.squeeze_(0)\n",
    "std2.squeeze_(0)\n",
    "print(mean.shape)\n",
    "\n",
    "# Calculating RMSE for the first 100 timesteps\n",
    "actual_values = dset.data[1][:100, 1:].to(device)\n",
    "predicted_values = mean\n",
    "#append dset data 0 to actual values\n",
    "actual_values=torch.cat((actual_values,dset.data[0][:100, 1:].to(device)),dim=0)\n",
    "predicted_values_bad = mean2\n",
    "# Calculating RMSE\n",
    "rmse = torch.sqrt(torch.mean((actual_values - predicted_values) ** 2))\n",
    "rmse_bad = torch.sqrt(torch.mean((actual_values - predicted_values_bad) ** 2))\n",
    "\n",
    "print(rmse.item(),rmse_bad.item())  # Converting the tensor to a Python float for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "# Let's assume that 'mean' is the model's output for the keypoints predictions\n",
    "# and has a shape of [200, 34] (for 200 timesteps and 17 keypoints * 2 coordinates each)\n",
    "# Define the connections between keypoints for the stick figure\n",
    "# These connections are based on the COCO-format keypoint definition provided\n",
    "skeleton_connections = [\n",
    "    (0, 1), (0, 2), (1, 3), (2, 4), # Head to ears\n",
    "    (5, 6), (5, 7), (7, 9), # Left arm\n",
    "    (6, 8), (8, 10), # Right arm\n",
    "    (11, 12), (11, 13), (13, 15), # Left leg\n",
    "    (12, 14), (14, 16), # Right leg\n",
    "    (5, 11), (6, 12), # Body (shoulder to hip)\n",
    "    (5, 6), (11, 12) # Shoulders and hips\n",
    "]\n",
    "\n",
    "# COCO-format keypoint definition\n",
    "keypoint_labels = [\n",
    "    \"nose\", \n",
    "    \"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\", \n",
    "    \"left_shoulder\", \"right_shoulder\", \n",
    "    \"left_elbow\", \"right_elbow\", \n",
    "    \"left_wrist\", \"right_wrist\", \n",
    "    \"left_hip\", \"right_hip\", \n",
    "    \"left_knee\", \"right_knee\", \n",
    "    \"left_ankle\", \"right_ankle\"\n",
    "]\n",
    "# Reshape the model's output to match the original keypoints' shape\n",
    "predicted_keypoints = mean2.cpu().numpy().reshape(-1, 17, 2)  # Adjust shape as per your data\n",
    "\n",
    "# The rest of the animation code would be similar to your existing code.\n",
    "# You would use 'predicted_keypoints' instead of 'reduced_frames_keypoints' to update the animation.\n",
    "def animate_predictions(predictions):\n",
    "    # Reshape the predictions to the original keypoints' shape if necessary\n",
    "    reshaped_predictions = predictions.reshape(-1, 17, 2)\n",
    "\n",
    "    # Create a new figure for the 2D animation\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Initialize the lines and annotations for the animation\n",
    "    lines = [ax.plot([], [], 'ro-')[0] for _ in skeleton_connections]\n",
    "    annotations = [ax.text(0, 0, keypoint_labels[i], color='blue') for i in range(len(keypoint_labels))]\n",
    "\n",
    "    # Set the plot limits\n",
    "    ax.set_xlim(np.min(reshaped_predictions[:,:,0]), np.max(reshaped_predictions[:,:,0]))\n",
    "    ax.set_ylim(np.min(reshaped_predictions[:,:,1]), np.max(reshaped_predictions[:,:,1]))\n",
    "\n",
    "    # Set the labels for the axes\n",
    "    ax.set_xlabel('X axis')\n",
    "    ax.set_ylabel('Y axis')\n",
    "\n",
    "    # Define the update function for the animation\n",
    "    def update(frame_number):\n",
    "        for line, connection in zip(lines, skeleton_connections):\n",
    "            point1 = reshaped_predictions[frame_number][connection[0], :]\n",
    "            point2 = reshaped_predictions[frame_number][connection[1], :]\n",
    "            line.set_data([point1[0], point2[0]], [point1[1], point2[1]])\n",
    "        for annotation, keypoint in zip(annotations, reshaped_predictions[frame_number]):\n",
    "            annotation.set_position((keypoint[0], keypoint[1]))\n",
    "        return lines + annotations\n",
    "\n",
    "    # Creating the 2D animation\n",
    "    ani = FuncAnimation(fig, update, frames=len(reshaped_predictions),\n",
    "                        interval=500/60, blit=True)\n",
    "\n",
    "    # Display the animation\n",
    "    plt.show()\n",
    "\n",
    "    return ani\n",
    "\n",
    "# Call the function with your model's output\n",
    "animation = animate_predictions(predicted_keypoints)\n",
    "\n",
    "# If you want to save the animation, you can do so with the save method:\n",
    "animation.save('predicted_animation_bad_0_6_slow.mp4', writer='ffmpeg', fps=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 35])\n"
     ]
    }
   ],
   "source": [
    "# Extract the first trajectory\n",
    "first_trajectory = dset.data[0]\n",
    "print(first_trajectory.shape)  # Should be (200, 34) for 2d case\n",
    "\n",
    "\n",
    "# Assuming the first column is time and the rest are keypoint dimensions\n",
    "time = first_trajectory[:, 0]  # First column for time\n",
    "keypoint_data = first_trajectory[:, 1:]  # The remaining columns for keypoints\n",
    "\n",
    "# Plot each dimension of the keypoints across time\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i in range(keypoint_data.shape[1]):\n",
    "    plt.plot(time, keypoint_data[:, i], label=f'Keypoint {i+1}')\n",
    "\n",
    "plt.title('Keypoint Dimensions Through Time for the First Trajectory')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Keypoint Value')\n",
    "#plt.legend()  # This may create a very large legend if there are many keypoints. Consider commenting this out if it's too cluttered.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 9, 52]) torch.Size([16, 16, 52]) torch.Size([16, 9]) torch.Size([16, 16])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_t, y_t, x_mask, y_mask = dset.get_sample(16, max_context=10, max_target=10)\n",
    "print(x_t.shape, y_t.shape, x_mask.shape, y_mask.shape)\n",
    "for i in range(8):\n",
    "    plt.scatter(x_t[i, :, 0], x_t[i, :, 1], marker=\"o\", c=\"b\", alpha=0.7)\n",
    "    plt.scatter(y_t[i, :, 0], y_t[i, :, 1], marker=\"x\", c=\"r\", alpha=0.7)\n",
    "    # plt.xlim((-2.2, 2.2))\n",
    "# plt.ylim((-2.2, 2.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: 5.01585888671875\n",
      "Step 1000, loss: 205.3786867852211\n",
      "Step 2000, loss: 3.35062646484375\n",
      "Step 3000, loss: 3.3479283187389375\n",
      "Step 4000, loss: 3.3481773660182954\n",
      "Step 5000, loss: 3.3464944353103636\n",
      "Step 6000, loss: 3.3478577687740327\n",
      "Step 7000, loss: 3.3482301986217498\n",
      "Step 8000, loss: 3.349679735660553\n",
      "Step 9000, loss: 3.348268180131912\n",
      "Step 10000, loss: 3.3495208220481874\n",
      "Step 11000, loss: 3.3493617689609527\n",
      "Step 12000, loss: 3.349293830156326\n",
      "Step 13000, loss: 3.3485396156311036\n",
      "Step 14000, loss: 3.3497390632629394\n",
      "Step 15000, loss: 3.349035743713379\n",
      "Step 16000, loss: 3.34944424033165\n",
      "Step 17000, loss: 3.3485634398460387\n",
      "Step 18000, loss: 3.3494696044921874\n",
      "Step 19000, loss: 3.349708419084549\n",
      "Step 20000, loss: 3.349815585374832\n",
      "Step 21000, loss: 3.35060160279274\n",
      "Step 22000, loss: 3.348719444990158\n",
      "Step 23000, loss: 3.348776112794876\n",
      "Step 24000, loss: 3.343698935031891\n",
      "Step 25000, loss: 3.3176519095897676\n",
      "Step 26000, loss: 3.232186873435974\n",
      "Step 27000, loss: 3.220133108615875\n",
      "Step 28000, loss: 3.202160613536835\n",
      "Step 29000, loss: 3.1634923701286315\n",
      "Step 30000, loss: 3.08693611741066\n",
      "Step 31000, loss: 2.964595249414444\n",
      "Step 32000, loss: 2.8483795874118805\n",
      "Step 33000, loss: 2.736026686191559\n",
      "Step 34000, loss: 2.6385252935886383\n",
      "Step 35000, loss: 2.5818323781490324\n",
      "Step 36000, loss: 2.5309076514244078\n",
      "Step 37000, loss: 2.4971629936695097\n",
      "Step 38000, loss: 2.4577090227603913\n",
      "Step 39000, loss: 2.432597369670868\n",
      "Step 40000, loss: 2.370719628572464\n",
      "Step 41000, loss: 2.3130083873271943\n",
      "Step 42000, loss: 2.240867654085159\n",
      "Step 43000, loss: 2.1899354367256163\n",
      "Step 44000, loss: 2.165348578929901\n",
      "Step 45000, loss: 2.1232956969738006\n",
      "Step 46000, loss: 2.100497255206108\n",
      "Step 47000, loss: 2.0780302304029465\n",
      "Step 48000, loss: 2.055491121888161\n",
      "Step 49000, loss: 2.0382427105903624\n",
      "Step 50000, loss: 2.035179882645607\n",
      "Step 51000, loss: 2.003428397655487\n",
      "Step 52000, loss: 1.986870210647583\n",
      "Step 53000, loss: 1.9938343315124512\n",
      "Step 54000, loss: 1.964501096367836\n",
      "Step 55000, loss: 1.9500689132213593\n",
      "Step 56000, loss: 1.9397098050117492\n",
      "Step 57000, loss: 1.9273059533834458\n",
      "Step 58000, loss: 1.91649744617939\n",
      "Step 59000, loss: 1.9102395358085633\n",
      "Step 60000, loss: 1.8932504694461822\n",
      "Step 61000, loss: 1.9268325057029725\n",
      "Step 62000, loss: 1.902794215798378\n",
      "Step 63000, loss: 1.8847106186151505\n",
      "Step 64000, loss: 1.8521988701820373\n",
      "Step 65000, loss: 1.8698721704483032\n",
      "Step 66000, loss: 1.8509173538684844\n",
      "Step 67000, loss: 1.8254398033618926\n",
      "Step 68000, loss: 1.8449289087057115\n",
      "Step 69000, loss: 1.8016933327913285\n",
      "Step 70000, loss: 1.803879787683487\n",
      "Step 71000, loss: 1.8251601362228393\n",
      "Step 72000, loss: 1.8191756989955903\n",
      "Step 73000, loss: 1.7656027274131776\n",
      "Step 74000, loss: 1.785824420928955\n",
      "Step 75000, loss: 1.7797903459072113\n",
      "Step 76000, loss: 1.7713938381671905\n",
      "Step 77000, loss: 1.7603592125177383\n",
      "Step 78000, loss: 1.7660671855211258\n",
      "Step 79000, loss: 1.7476435406208037\n",
      "Step 80000, loss: 1.747720403432846\n",
      "Step 81000, loss: 1.710662071943283\n",
      "Step 82000, loss: 1.7505867587327957\n",
      "Step 83000, loss: 1.7007443791627883\n",
      "Step 84000, loss: 1.7088618314266204\n",
      "Step 85000, loss: 1.7053602105379104\n",
      "Step 86000, loss: 1.6921719427108766\n",
      "Step 87000, loss: 1.6823603925704955\n",
      "Step 88000, loss: 1.6758645235300065\n",
      "Step 89000, loss: 1.670256614089012\n",
      "Step 90000, loss: 1.6759179807901383\n",
      "Step 91000, loss: 1.6818705238103866\n",
      "Step 92000, loss: 1.7287284413576125\n",
      "Step 93000, loss: 1.6591245465278626\n",
      "Step 94000, loss: 1.6688103820085525\n",
      "Step 95000, loss: 1.650305475115776\n",
      "Step 96000, loss: 1.6667239271402359\n",
      "Step 97000, loss: 1.6441169811487197\n",
      "Step 98000, loss: 1.6397451287508011\n",
      "Step 99000, loss: 1.6255245370864868\n",
      "Step 100000, loss: 1.6174427741765975\n",
      "Step 101000, loss: 1.6313224049806594\n",
      "Step 102000, loss: 1.6236278860569\n",
      "Step 103000, loss: 1.6026630979776382\n",
      "Step 104000, loss: 1.6054860241413116\n",
      "Step 105000, loss: 1.603323139309883\n",
      "Step 106000, loss: 1.6909495984315872\n",
      "Step 107000, loss: 1.5946243230104447\n",
      "Step 108000, loss: 1.713821571111679\n",
      "Step 109000, loss: 1.5796083170175552\n",
      "Step 110000, loss: 1.6070722233057022\n",
      "Step 111000, loss: 1.5907760580778123\n",
      "Step 112000, loss: 1.5967461161613465\n",
      "Step 113000, loss: 1.6268264770507812\n",
      "Step 114000, loss: 1.5965590561628342\n",
      "Step 115000, loss: 1.6306197128295898\n",
      "Step 116000, loss: 1.5559102367162705\n",
      "Step 117000, loss: 1.656144477248192\n",
      "Step 118000, loss: 1.5544835294485093\n",
      "Step 119000, loss: 1.5568511856794358\n",
      "Step 120000, loss: 1.5709001379013061\n",
      "Step 121000, loss: 1.620096072077751\n",
      "Step 122000, loss: 1.5379037313461303\n",
      "Step 123000, loss: 1.5394584139585494\n",
      "Step 124000, loss: 1.52723855137825\n",
      "Step 125000, loss: 1.5387746289968491\n",
      "Step 126000, loss: 1.503843715786934\n",
      "Step 127000, loss: 1.5173920571804047\n",
      "Step 128000, loss: 1.4878624529838562\n",
      "Step 129000, loss: 1.5172675650119782\n",
      "Step 130000, loss: 1.5372790105342864\n",
      "Step 131000, loss: 1.4924286056756972\n",
      "Step 132000, loss: 1.548975730061531\n",
      "Step 133000, loss: 1.5005578513145448\n",
      "Step 134000, loss: 1.4852295823097228\n",
      "Step 135000, loss: 1.5421460320949554\n",
      "Step 136000, loss: 1.4679668401479722\n",
      "Step 137000, loss: 1.518348844766617\n",
      "Step 138000, loss: 1.4584509432315826\n",
      "Step 139000, loss: 1.4364788055419921\n",
      "Step 140000, loss: 1.4821010024547576\n",
      "Step 141000, loss: 1.4527575186491013\n",
      "Step 142000, loss: 1.438222251176834\n",
      "Step 143000, loss: 1.5066918128728866\n",
      "Step 144000, loss: 1.4532425248622893\n",
      "Step 145000, loss: 1.4392615706920624\n",
      "Step 146000, loss: 1.4556371556520462\n",
      "Step 147000, loss: 1.44413460791111\n",
      "Step 148000, loss: 1.4432379999160767\n",
      "Step 149000, loss: 1.5145394530296326\n",
      "Step 150000, loss: 1.442492675304413\n",
      "Step 151000, loss: 1.404659442305565\n",
      "Step 152000, loss: 1.4001294928789139\n",
      "Step 153000, loss: 1.3880159516334534\n",
      "Step 154000, loss: 1.410790325641632\n",
      "Step 155000, loss: 1.406338987827301\n",
      "Step 156000, loss: 1.4625471370220184\n",
      "Step 157000, loss: 1.4014773302078247\n",
      "Step 158000, loss: 1.4044822996854782\n",
      "Step 159000, loss: 1.4125291720628739\n",
      "Step 160000, loss: 1.3851354521512986\n",
      "Step 161000, loss: 1.3987246160507203\n",
      "Step 162000, loss: 1.3526598616838454\n",
      "Step 163000, loss: 1.4748008638620376\n",
      "Step 164000, loss: 1.3441596610546112\n",
      "Step 165000, loss: 1.3808111178874969\n",
      "Step 166000, loss: 1.3772046533823012\n",
      "Step 167000, loss: 1.4339926640987397\n",
      "Step 168000, loss: 1.3617745139598847\n",
      "Step 169000, loss: 1.4703536986112595\n",
      "Step 170000, loss: 1.3359865243434905\n",
      "Step 171000, loss: 1.3647982034683228\n",
      "Step 172000, loss: 1.358445605099201\n",
      "Step 173000, loss: 1.3751891260147096\n",
      "Step 174000, loss: 1.3746420543193818\n",
      "Step 175000, loss: 1.3810657876729966\n",
      "Step 176000, loss: 1.374373635172844\n",
      "Step 177000, loss: 1.4540025281906128\n",
      "Step 178000, loss: 1.295525308251381\n",
      "Step 179000, loss: 1.329218967139721\n",
      "Step 180000, loss: 1.4200685027837754\n",
      "Step 181000, loss: 1.3253600206375122\n",
      "Step 182000, loss: 1.406144103884697\n",
      "Step 183000, loss: 1.323624514222145\n",
      "Step 184000, loss: 1.2784247243404387\n",
      "Step 185000, loss: 1.3811414880752564\n",
      "Step 186000, loss: 1.2685008161067963\n",
      "Step 187000, loss: 1.2673036071062087\n",
      "Step 188000, loss: 1.3614762113690377\n",
      "Step 189000, loss: 1.2495140126347541\n",
      "Step 190000, loss: 1.311502557694912\n",
      "Step 191000, loss: 1.2396879653334618\n",
      "Step 192000, loss: 1.3834965004324913\n",
      "Step 193000, loss: 1.3645993956923486\n",
      "Step 194000, loss: 1.2538720924854279\n",
      "Step 195000, loss: 1.3565991971492768\n",
      "Step 196000, loss: 1.252065855383873\n",
      "Step 197000, loss: 1.2887152012586593\n",
      "Step 198000, loss: 1.2531320722699166\n",
      "Step 199000, loss: 1.2636232176423072\n",
      "Step 200000, loss: 1.288864865899086\n",
      "Step 201000, loss: 1.2146622814536094\n",
      "Step 202000, loss: 1.2386024976968766\n",
      "Step 203000, loss: 1.2465872515439986\n",
      "Step 204000, loss: 1.2831986804008484\n",
      "Step 205000, loss: 1.2685077766180037\n",
      "Step 206000, loss: 1.1963876389861108\n",
      "Step 207000, loss: 1.3188587367534637\n",
      "Step 208000, loss: 1.2384759649038315\n",
      "Step 209000, loss: 1.1648214147090912\n",
      "Step 210000, loss: 1.3189323846101761\n",
      "Step 211000, loss: 1.3517205150127412\n",
      "Step 212000, loss: 1.1854700198173522\n",
      "Step 213000, loss: 1.1735669070482253\n",
      "Step 214000, loss: 1.2330661091804505\n",
      "Step 215000, loss: 1.255900608241558\n",
      "Step 216000, loss: 1.1696692455410957\n",
      "Step 217000, loss: 1.2234074892997742\n",
      "Step 218000, loss: 1.1703857806921005\n",
      "Step 219000, loss: 1.1395452165007591\n",
      "Step 220000, loss: 1.2102849680781365\n",
      "Step 221000, loss: 1.2830832347273826\n",
      "Step 222000, loss: 1.1299161526560784\n",
      "Step 223000, loss: 1.1674203839302062\n",
      "Step 224000, loss: 1.1643831639885902\n",
      "Step 225000, loss: 1.206394061088562\n",
      "Step 226000, loss: 1.1017085931301116\n",
      "Step 227000, loss: 1.195691767156124\n",
      "Step 228000, loss: 1.230299400448799\n",
      "Step 229000, loss: 1.1504526135325432\n",
      "Step 230000, loss: 1.2027438036203384\n",
      "Step 231000, loss: 1.1530726829767226\n",
      "Step 232000, loss: 1.1227678805589676\n",
      "Step 233000, loss: 1.1610787585377693\n",
      "Step 234000, loss: 1.1122085003852844\n",
      "Step 235000, loss: 1.0852405912876129\n",
      "Step 236000, loss: 1.1403523184657096\n",
      "Step 237000, loss: 1.1010322456359862\n",
      "Step 238000, loss: 1.136007325708866\n",
      "Step 239000, loss: 1.2274243469238282\n",
      "Step 240000, loss: 1.1472382753491401\n",
      "Step 241000, loss: 1.1581729789376258\n",
      "Step 242000, loss: 1.091935787498951\n",
      "Step 243000, loss: 1.1279370371699333\n",
      "Step 244000, loss: 1.175191688001156\n",
      "Step 245000, loss: 1.0804845006465913\n",
      "Step 246000, loss: 1.0888645757436752\n",
      "Step 247000, loss: 1.0789450839161874\n",
      "Step 248000, loss: 1.0478255867362023\n",
      "Step 249000, loss: 1.0681269691586495\n",
      "Step 250000, loss: 1.028206023812294\n",
      "Step 251000, loss: 1.0658414466381072\n",
      "Step 252000, loss: 1.021578834116459\n",
      "Step 253000, loss: 1.260793563246727\n",
      "Step 254000, loss: 1.0680114637613296\n",
      "Step 255000, loss: 1.099709664583206\n",
      "Step 256000, loss: 1.0468012884259223\n",
      "Step 257000, loss: 1.0022177302837372\n",
      "Step 258000, loss: 1.0561952494978906\n",
      "Step 259000, loss: 1.1085639211535454\n",
      "Step 260000, loss: 1.139852988898754\n",
      "Step 261000, loss: 1.1899264101982117\n",
      "Step 262000, loss: 1.0655806214809418\n",
      "Step 263000, loss: 1.052021487057209\n",
      "Step 264000, loss: 0.984032707452774\n",
      "Step 265000, loss: 1.0850884979963302\n",
      "Step 266000, loss: 1.0081500825881957\n",
      "Step 267000, loss: 1.0623455967307092\n",
      "Step 268000, loss: 1.0579970028996468\n",
      "Step 269000, loss: 1.0775302644371987\n",
      "Step 270000, loss: 0.9633631531000137\n",
      "Step 271000, loss: 1.0475300295352936\n",
      "Step 272000, loss: 1.0765614619255066\n",
      "Step 273000, loss: 1.1073206719756126\n",
      "Step 274000, loss: 1.0745922752022743\n",
      "Step 275000, loss: 1.049697112441063\n",
      "Step 276000, loss: 0.9621343333125114\n",
      "Step 277000, loss: 1.0353390591740608\n",
      "Step 278000, loss: 1.0128251974582672\n",
      "Step 279000, loss: 1.0337779403924943\n",
      "Step 280000, loss: 0.9905230954885483\n",
      "Step 281000, loss: 0.9288633708953857\n",
      "Step 282000, loss: 1.018090019404888\n",
      "Step 283000, loss: 1.0624483223557473\n",
      "Step 284000, loss: 0.9882255223989487\n",
      "Step 285000, loss: 1.0143144828677177\n",
      "Step 286000, loss: 0.935840567946434\n",
      "Step 287000, loss: 0.9403102061748505\n",
      "Step 288000, loss: 0.9912310951352119\n",
      "Step 289000, loss: 0.9667057583928108\n",
      "Step 290000, loss: 0.9156350467205048\n",
      "Step 291000, loss: 0.9618793558478356\n",
      "Step 292000, loss: 1.0178902545571327\n",
      "Step 293000, loss: 1.09157010948658\n",
      "Step 294000, loss: 0.9908601129055024\n",
      "Step 295000, loss: 0.9375853782594203\n",
      "Step 296000, loss: 0.9118762249946595\n",
      "Step 297000, loss: 0.9400780289769173\n",
      "Step 298000, loss: 0.9938595383763313\n",
      "Step 299000, loss: 1.0839914233088492\n",
      "Step 300000, loss: 0.969639363348484\n",
      "Step 301000, loss: 0.8727678438127041\n",
      "Step 302000, loss: 1.1131339728236198\n",
      "Step 303000, loss: 0.9595688253045082\n",
      "Step 304000, loss: 0.9308476991057396\n",
      "Step 305000, loss: 0.9093882115483284\n",
      "Step 306000, loss: 1.110456359386444\n",
      "Step 307000, loss: 0.9095371453762054\n",
      "Step 308000, loss: 0.9753000553250313\n",
      "Step 309000, loss: 0.8640194734334946\n",
      "Step 310000, loss: 0.8845346218943596\n",
      "Step 311000, loss: 0.9159740725755692\n",
      "Step 312000, loss: 0.9554991230964661\n",
      "Step 313000, loss: 0.918626164585352\n",
      "Step 314000, loss: 0.9273544389605523\n",
      "Step 315000, loss: 0.9222069469690323\n",
      "Step 316000, loss: 0.8588636960685253\n",
      "Step 317000, loss: 0.8215293079912662\n",
      "Step 318000, loss: 0.9154314356446266\n",
      "Step 319000, loss: 0.805263918787241\n",
      "Step 320000, loss: 0.8705190121233464\n",
      "Step 321000, loss: 0.913801225811243\n",
      "Step 322000, loss: 0.8567737572491169\n",
      "Step 323000, loss: 0.7933457861542702\n",
      "Step 324000, loss: 0.9248706293702126\n",
      "Step 325000, loss: 0.8599117949306965\n",
      "Step 326000, loss: 0.811479826927185\n",
      "Step 327000, loss: 0.933263427913189\n",
      "Step 328000, loss: 0.8577488937079907\n",
      "Step 329000, loss: 0.8000681731402874\n",
      "Step 330000, loss: 0.7852930600941181\n",
      "Step 331000, loss: 0.8516184156537056\n",
      "Step 332000, loss: 0.8658362086415291\n",
      "Step 333000, loss: 0.7880449992120266\n",
      "Step 334000, loss: 0.927371109008789\n",
      "Step 335000, loss: 0.7780818791687488\n",
      "Step 336000, loss: 0.8853757758140564\n",
      "Step 337000, loss: 0.8417388843595982\n",
      "Step 338000, loss: 0.8040595040917397\n",
      "Step 339000, loss: 0.8209474581778049\n",
      "Step 340000, loss: 0.7925852822959423\n",
      "Step 341000, loss: 0.8896180095672608\n",
      "Step 342000, loss: 0.7995678140819072\n",
      "Step 343000, loss: 0.8664313855171204\n",
      "Step 344000, loss: 0.8606638559401035\n",
      "Step 345000, loss: 0.7842302875518798\n",
      "Step 346000, loss: 0.8604868791699409\n",
      "Step 347000, loss: 0.7945659585893154\n",
      "Step 348000, loss: 0.7444575256407261\n",
      "Step 349000, loss: 0.8087382234036923\n",
      "Step 350000, loss: 0.7418808031380176\n",
      "Step 351000, loss: 0.7500257660150528\n",
      "Step 352000, loss: 0.7589877110123634\n",
      "Step 353000, loss: 0.7212630116045475\n",
      "Step 354000, loss: 0.8458396612107754\n",
      "Step 355000, loss: 0.8368126084208488\n",
      "Step 356000, loss: 0.8234846098721027\n",
      "Step 357000, loss: 0.7521763205230236\n",
      "Step 358000, loss: 0.7678572600185871\n",
      "Step 359000, loss: 0.9085069192945957\n",
      "Step 360000, loss: 0.9935170418024063\n",
      "Step 361000, loss: 0.7997930878102779\n",
      "Step 362000, loss: 0.7584974738955498\n",
      "Step 363000, loss: 0.8266090212464332\n",
      "Step 364000, loss: 0.7545778496265412\n",
      "Step 365000, loss: 0.7202136509120465\n",
      "Step 366000, loss: 0.8392562137246132\n",
      "Step 367000, loss: 0.8109085768461227\n",
      "Step 368000, loss: 0.7002411521375179\n",
      "Step 369000, loss: 1.0074331339299678\n",
      "Step 370000, loss: 0.7451986967623234\n",
      "Step 371000, loss: 0.7905087693333626\n",
      "Step 372000, loss: 0.7657221519052982\n",
      "Step 373000, loss: 0.6981044939458371\n",
      "Step 374000, loss: 0.7200870778262615\n",
      "Step 375000, loss: 0.7997687801420689\n",
      "Step 376000, loss: 0.8845943977236748\n",
      "Step 377000, loss: 0.642571366161108\n",
      "Step 378000, loss: 0.7246270267367363\n",
      "Step 379000, loss: 0.7128026001155376\n",
      "Step 380000, loss: 0.7649062000215053\n",
      "Step 381000, loss: 0.7540887095034122\n",
      "Step 382000, loss: 0.7843655911386013\n",
      "Step 383000, loss: 0.7355435734987259\n",
      "Step 384000, loss: 0.7226345032900572\n",
      "Step 385000, loss: 0.7855594161450863\n",
      "Step 386000, loss: 0.730953655436635\n",
      "Step 387000, loss: 0.6764524795114994\n",
      "Step 388000, loss: 0.6536268537044525\n",
      "Step 389000, loss: 0.6914579863846302\n",
      "Step 390000, loss: 0.8746411064565182\n",
      "Step 391000, loss: 0.6211125894784927\n",
      "Step 392000, loss: 0.5944684172421694\n",
      "Step 393000, loss: 0.7678357520699501\n",
      "Step 394000, loss: 0.7703522378057241\n",
      "Step 395000, loss: 0.6273656058013439\n",
      "Step 396000, loss: 0.7629552459567785\n",
      "Step 397000, loss: 0.6341293962150812\n",
      "Step 398000, loss: 0.6918340904116631\n",
      "Step 399000, loss: 0.702487786307931\n",
      "Step 400000, loss: 0.6623026243746281\n",
      "Step 401000, loss: 0.749197564676404\n",
      "Step 402000, loss: 0.7595554881244898\n",
      "Step 403000, loss: 0.7601515568196774\n",
      "Step 404000, loss: 0.6291462086737156\n",
      "Step 405000, loss: 0.6252721748054028\n",
      "Step 406000, loss: 0.7442276954054833\n",
      "Step 407000, loss: 0.7139179208576679\n",
      "Step 408000, loss: 0.6643659155368805\n",
      "Step 409000, loss: 0.7981776144206524\n",
      "Step 410000, loss: 0.6275228665918112\n",
      "Step 411000, loss: 0.6783878116458655\n",
      "Step 412000, loss: 0.6435442058444023\n",
      "Step 413000, loss: 0.630747848957777\n",
      "Step 414000, loss: 0.7546311223804951\n",
      "Step 415000, loss: 0.8023193450123072\n",
      "Step 416000, loss: 0.7277352906018496\n",
      "Step 417000, loss: 0.6254895675629377\n",
      "Step 418000, loss: 0.685951356291771\n",
      "Step 419000, loss: 0.7665546295195818\n",
      "Step 420000, loss: 0.6665615240186453\n",
      "Step 421000, loss: 0.7003545472621918\n",
      "Step 422000, loss: 0.8939386831521988\n",
      "Step 423000, loss: 0.5387775546610355\n",
      "Step 424000, loss: 0.6003439950197935\n",
      "Step 425000, loss: 0.6641168148815632\n",
      "Step 426000, loss: 0.6680827183276414\n",
      "Step 427000, loss: 0.7136195621788501\n",
      "Step 428000, loss: 0.602417542397976\n",
      "Step 429000, loss: 0.6723958857133985\n",
      "Step 430000, loss: 0.6946945872157813\n",
      "Step 431000, loss: 0.5919551446065306\n",
      "Step 432000, loss: 0.5393384817242622\n",
      "Step 433000, loss: 0.5987267566770316\n",
      "Step 434000, loss: 0.6095455406010151\n",
      "Step 435000, loss: 0.6302734277695418\n",
      "Step 436000, loss: 0.5779876278862357\n",
      "Step 437000, loss: 0.641279841452837\n",
      "Step 438000, loss: 0.6402337924540042\n",
      "Step 439000, loss: 0.6803970689177513\n",
      "Step 440000, loss: 0.5492911383956671\n",
      "Step 441000, loss: 0.5404858395010232\n",
      "Step 442000, loss: 0.6510124285370111\n",
      "Step 443000, loss: 0.5997393089532852\n",
      "Step 444000, loss: 0.6521454392597079\n",
      "Step 445000, loss: 0.5764270533770323\n",
      "Step 446000, loss: 0.5961606450006366\n",
      "Step 447000, loss: 0.4875210459791124\n",
      "Step 448000, loss: 0.7320611174851656\n",
      "Step 449000, loss: 0.48426754656434057\n",
      "Step 450000, loss: 0.664336752038449\n",
      "Step 451000, loss: 0.6704009087458253\n",
      "Step 452000, loss: 0.5119038125276566\n",
      "Step 453000, loss: 0.6324657084867358\n",
      "Step 454000, loss: 0.5876380224153399\n",
      "Step 455000, loss: 0.7046442677900195\n",
      "Step 456000, loss: 0.4800426439344883\n",
      "Step 457000, loss: 0.5830428606122732\n",
      "Step 458000, loss: 0.610449684266001\n",
      "Step 459000, loss: 0.6558728603757917\n",
      "Step 460000, loss: 0.5226014320552349\n",
      "Step 461000, loss: 0.7234730264395476\n",
      "Step 462000, loss: 0.6573152545914054\n",
      "Step 463000, loss: 0.5511227752193808\n",
      "Step 464000, loss: 0.5365483033657074\n",
      "Step 465000, loss: 0.6996171924769878\n",
      "Step 466000, loss: 0.6643066731914878\n",
      "Step 467000, loss: 0.525080653719604\n",
      "Step 468000, loss: 0.5774409906715154\n",
      "Step 469000, loss: 0.5823478899747133\n",
      "Step 470000, loss: 0.5765285950377583\n",
      "Step 471000, loss: 0.6649604072123766\n",
      "Step 472000, loss: 0.5708036811351777\n",
      "Step 473000, loss: 0.6341064871829003\n",
      "Step 474000, loss: 0.5283261861205101\n",
      "Step 475000, loss: 0.721300523635\n",
      "Step 476000, loss: 0.5557263103909791\n",
      "Step 477000, loss: 0.6221161900875158\n",
      "Step 478000, loss: 0.6612324520237743\n",
      "Step 479000, loss: 0.49243197015300394\n",
      "Step 480000, loss: 0.5471619092244655\n",
      "Step 481000, loss: 0.7437255853433162\n",
      "Step 482000, loss: 0.5465339685138315\n",
      "Step 483000, loss: 0.43881681486568413\n",
      "Step 484000, loss: 0.5254166137650609\n",
      "Step 485000, loss: 0.7602911258926615\n",
      "Step 486000, loss: 0.42861576613038777\n",
      "Step 487000, loss: 0.5337202743347734\n",
      "Step 488000, loss: 0.5493706082524732\n",
      "Step 489000, loss: 0.4261728790542111\n",
      "Step 490000, loss: 0.5082755395714194\n",
      "Step 491000, loss: 0.44688305433280767\n",
      "Step 492000, loss: 0.5552356757996604\n",
      "Step 493000, loss: 0.4646203556349501\n",
      "Step 494000, loss: 0.5594339304808528\n",
      "Step 495000, loss: 0.5677368736714125\n",
      "Step 496000, loss: 0.5439376140739769\n",
      "Step 497000, loss: 0.46481283749919383\n",
      "Step 498000, loss: 0.4904327613050118\n",
      "Step 499000, loss: 0.6547474971843112\n",
      "Step 500000, loss: 0.5335681302687153\n",
      "Step 501000, loss: 0.5023448146197479\n",
      "Step 502000, loss: 0.4655452897411305\n",
      "Step 503000, loss: 0.39537329273996874\n",
      "Step 504000, loss: 0.37521726739313455\n",
      "Step 505000, loss: 0.6456477302908897\n",
      "Step 506000, loss: 0.4110304689182085\n",
      "Step 507000, loss: 0.576520205126144\n",
      "Step 508000, loss: 0.5725399989141151\n",
      "Step 509000, loss: 0.6028654353006277\n",
      "Step 510000, loss: 0.43605388183088506\n",
      "Step 511000, loss: 0.5725584215396083\n",
      "Step 512000, loss: 0.5294292925370392\n",
      "Step 513000, loss: 0.42198636851878835\n",
      "Step 514000, loss: 0.4545796684600646\n",
      "Step 515000, loss: 0.4528165311912308\n",
      "Step 516000, loss: 0.5519098332729191\n",
      "Step 517000, loss: 0.41150371544482184\n",
      "Step 518000, loss: 0.4264160330552404\n",
      "Step 519000, loss: 0.4037984637416666\n",
      "Step 520000, loss: 0.4404042937116174\n",
      "Step 521000, loss: 0.645606043376727\n",
      "Step 522000, loss: 0.3678533175244229\n",
      "Step 523000, loss: 0.5510765137348208\n",
      "Step 524000, loss: 0.45019246858111\n",
      "Step 525000, loss: 0.47392682410314135\n",
      "Step 526000, loss: 0.5096089063052205\n",
      "Step 527000, loss: 0.4413711251914501\n",
      "Step 528000, loss: 0.40214212328417487\n",
      "Step 529000, loss: 0.5409590741976863\n",
      "Step 530000, loss: 0.4034484378240304\n",
      "Step 531000, loss: 0.5565206387754297\n",
      "Step 532000, loss: 0.34118927327511483\n",
      "Step 533000, loss: 0.486356842961628\n",
      "Step 534000, loss: 0.4128423491455033\n",
      "Step 535000, loss: 0.40359950111218496\n",
      "Step 536000, loss: 0.4710320659132558\n",
      "Step 537000, loss: 0.3585559454938921\n",
      "Step 538000, loss: 0.39324192158607185\n",
      "Step 539000, loss: 0.3528883322094916\n",
      "Step 540000, loss: 0.38800028322375146\n",
      "Step 541000, loss: 0.31367698052572085\n",
      "Step 542000, loss: 0.4813380627108854\n",
      "Step 543000, loss: 0.3384136760243564\n",
      "Step 544000, loss: 0.3532555367872119\n",
      "Step 545000, loss: 0.4927955911672907\n",
      "Step 546000, loss: 0.31204946976166686\n",
      "Step 547000, loss: 0.4385683824253938\n",
      "Step 548000, loss: 0.4237094454032631\n",
      "Step 549000, loss: 0.38124442614466536\n",
      "Step 550000, loss: 0.4028460837714738\n",
      "Step 551000, loss: 0.4678012434031116\n",
      "Step 552000, loss: 0.3550029326273361\n",
      "Step 553000, loss: 0.45241292196804717\n",
      "Step 554000, loss: 0.3314206911907531\n",
      "Step 555000, loss: 0.3786919907840784\n",
      "Step 556000, loss: 0.4503250086300977\n",
      "Step 557000, loss: 0.33256263970042343\n",
      "Step 558000, loss: 0.30305058271516466\n",
      "Step 559000, loss: 0.39671210930802864\n",
      "Step 560000, loss: 0.3289683730311317\n",
      "Step 561000, loss: 0.44572681152452426\n",
      "Step 562000, loss: 0.4866328323234429\n",
      "Step 563000, loss: 0.37782040685917306\n",
      "Step 564000, loss: 0.32553619945683115\n",
      "Step 565000, loss: 0.3209755288766319\n",
      "Step 566000, loss: 0.3727295812481607\n",
      "Step 567000, loss: 0.44086885102977974\n",
      "Step 568000, loss: 0.34410458321194165\n",
      "Step 569000, loss: 0.36199636807705976\n",
      "Step 570000, loss: 0.35967393316930973\n",
      "Step 571000, loss: 0.2571453907942341\n",
      "Step 572000, loss: 0.3893248509033219\n",
      "Step 573000, loss: 0.2837861310355074\n",
      "Step 574000, loss: 0.3124915203021228\n",
      "Step 575000, loss: 0.339655070514651\n",
      "Step 576000, loss: 0.3103341729557142\n",
      "Step 577000, loss: 0.36909547745694\n",
      "Step 578000, loss: 0.32274193242847105\n",
      "Step 579000, loss: 0.2872348875697935\n",
      "Step 580000, loss: 0.40015519334517013\n",
      "Step 581000, loss: 0.29036278505965313\n",
      "Step 582000, loss: 0.2723262786612886\n",
      "Step 583000, loss: 0.3945839252351143\n",
      "Step 584000, loss: 0.28808150868158555\n",
      "Step 585000, loss: 0.3444786716233066\n",
      "Step 586000, loss: 0.28807919765206313\n",
      "Step 587000, loss: 0.31864858174801336\n",
      "Step 588000, loss: 0.2964026384758181\n",
      "Step 589000, loss: 0.25859518425911665\n",
      "Step 590000, loss: 0.43628702577529477\n",
      "Step 591000, loss: 0.23620296975677774\n",
      "Step 592000, loss: 0.3337676449670689\n",
      "Step 593000, loss: 0.18874582771207496\n",
      "Step 594000, loss: 0.331515518653905\n",
      "Step 595000, loss: 0.2903345329181466\n",
      "Step 596000, loss: 0.25185789945715803\n",
      "Step 597000, loss: 0.317431194531724\n",
      "Step 598000, loss: 0.21022000257987203\n",
      "Step 599000, loss: 0.3875102087376435\n",
      "Step 600000, loss: 0.458761758863664\n",
      "Step 601000, loss: 0.2682827356477428\n",
      "Step 602000, loss: 0.18777268759834986\n",
      "Step 603000, loss: 0.24964087897521675\n",
      "Step 604000, loss: 0.3882429923652817\n",
      "Step 605000, loss: 0.16520501547548339\n",
      "Step 606000, loss: 0.41568460412156855\n",
      "Step 607000, loss: 0.286554901083593\n",
      "Step 608000, loss: 0.3362778912548383\n",
      "Step 609000, loss: 0.36222977897315284\n",
      "Step 610000, loss: 0.24264584640110842\n",
      "Step 611000, loss: 0.2900087170373663\n",
      "Step 612000, loss: 0.3346622682990856\n",
      "Step 613000, loss: 0.16956410912998035\n",
      "Step 614000, loss: 0.3368269192964581\n",
      "Step 615000, loss: 0.18315024140008063\n",
      "Step 616000, loss: 0.21318793005534098\n",
      "Step 617000, loss: 0.2795673417680955\n",
      "Step 618000, loss: 0.19372270700229274\n",
      "Step 619000, loss: 0.33441615102061767\n",
      "Step 620000, loss: 0.19262266429633745\n",
      "Step 621000, loss: 0.17511932024595445\n",
      "Step 622000, loss: 0.24506784616883123\n",
      "Step 623000, loss: 0.2580254072919488\n",
      "Step 624000, loss: 0.2190570229071891\n",
      "Step 625000, loss: 0.25224536463353436\n",
      "Step 626000, loss: 0.1619973334876995\n",
      "Step 627000, loss: 0.20237291617895245\n",
      "Step 628000, loss: 0.2134939247344446\n",
      "Step 629000, loss: 0.34022407766035756\n",
      "Step 630000, loss: 0.23945076699543277\n",
      "Step 631000, loss: 0.3111396538144909\n",
      "Step 632000, loss: 0.18385693987150445\n",
      "Step 633000, loss: 0.33906378274897\n",
      "Step 634000, loss: 0.1597987756082148\n",
      "Step 635000, loss: 0.260302201019098\n",
      "Step 636000, loss: 0.1850819530593144\n",
      "Step 637000, loss: 0.24919939073477143\n",
      "Step 638000, loss: 0.2443557906725182\n",
      "Step 639000, loss: 0.3189721256988996\n",
      "Step 640000, loss: 0.15761908423044224\n",
      "Step 641000, loss: 0.15269605669289013\n",
      "Step 642000, loss: 0.21145569262868957\n",
      "Step 643000, loss: 0.1845257334151538\n",
      "Step 644000, loss: 0.20204779942908135\n",
      "Step 645000, loss: 0.3797117237139828\n",
      "Step 646000, loss: 0.2072651774243568\n",
      "Step 647000, loss: 0.26074840264696103\n",
      "Step 648000, loss: 0.18131106487943907\n",
      "Step 649000, loss: 0.2575992359592347\n",
      "Step 650000, loss: 0.22449312325252685\n",
      "Step 651000, loss: 0.30621603823022453\n",
      "Step 652000, loss: 0.212128684684576\n",
      "Step 653000, loss: 0.15744701606151648\n",
      "Step 654000, loss: 0.31524243260224466\n",
      "Step 655000, loss: 0.11659946758858859\n",
      "Step 656000, loss: 0.3217089464477103\n",
      "Step 657000, loss: 0.14448288291469452\n",
      "Step 658000, loss: 0.22758632835917525\n",
      "Step 659000, loss: 0.14158152047581096\n",
      "Step 660000, loss: 0.2759354248069285\n",
      "Step 661000, loss: 0.3239009334555885\n",
      "Step 662000, loss: 0.1492995161928702\n",
      "Step 663000, loss: 0.22813227902224753\n",
      "Step 664000, loss: 0.20554222503199707\n",
      "Step 665000, loss: 0.24463257327429164\n",
      "Step 666000, loss: 0.2334202452801419\n",
      "Step 667000, loss: 0.19666249828392757\n",
      "Step 668000, loss: 0.19325466970349953\n",
      "Step 669000, loss: 0.1042085867180449\n",
      "Step 670000, loss: 0.31827196423824355\n",
      "Step 671000, loss: 0.17807693969692628\n",
      "Step 672000, loss: 0.2162246185876429\n",
      "Step 673000, loss: 0.15440684191769105\n",
      "Step 674000, loss: 0.29775650955410676\n",
      "Step 675000, loss: 0.18969359207176603\n",
      "Step 676000, loss: 0.14197976964974077\n",
      "Step 677000, loss: 0.2144398121105769\n",
      "Step 678000, loss: 0.29767373318124735\n",
      "Step 679000, loss: 0.23447055757345514\n",
      "Step 680000, loss: 0.2702768527251028\n",
      "Step 681000, loss: 0.2605015063543106\n",
      "Step 682000, loss: 0.2945943079752824\n",
      "Step 683000, loss: 0.37923049147045823\n",
      "Step 684000, loss: 0.15948176909853645\n",
      "Step 685000, loss: 0.32471504859661215\n",
      "Step 686000, loss: 0.25906564422053635\n",
      "Step 687000, loss: 0.11782310630439315\n",
      "Step 688000, loss: 0.37033700675296133\n",
      "Step 689000, loss: 0.35995624452031916\n",
      "Step 690000, loss: 0.26692308198893444\n",
      "Step 691000, loss: 0.19668906387090102\n",
      "Step 692000, loss: 0.17569780324983003\n",
      "Step 693000, loss: 0.4029612519466318\n",
      "Step 694000, loss: 0.18083470817506894\n",
      "Step 695000, loss: 0.4439584097439656\n",
      "Step 696000, loss: 0.31679066108417464\n",
      "Step 697000, loss: 0.1971107919061833\n",
      "Step 698000, loss: 0.3113024851886148\n",
      "Step 699000, loss: 0.19716905139120353\n",
      "Step 700000, loss: 0.29032492913994795\n",
      "Step 701000, loss: 0.19295039483561413\n",
      "Step 702000, loss: 0.14807019826643228\n",
      "Step 703000, loss: 0.08496463713695994\n",
      "Step 704000, loss: 0.24099924589721194\n",
      "Step 705000, loss: 0.15268615574779687\n",
      "Step 706000, loss: 0.25218267505901165\n",
      "Step 707000, loss: 0.1368864920733613\n",
      "Step 708000, loss: 0.12406811356230173\n",
      "Step 709000, loss: 0.22067566885845735\n",
      "Step 710000, loss: 0.04185666079598013\n",
      "Step 711000, loss: 0.22560505929560168\n",
      "Step 712000, loss: 0.27782879095433965\n",
      "Step 713000, loss: 0.07437804371782113\n",
      "Step 714000, loss: 0.21875358974240225\n",
      "Step 715000, loss: 0.08277935204579262\n",
      "Step 716000, loss: 0.08679875044917572\n",
      "Step 717000, loss: 0.29847872348566307\n",
      "Step 718000, loss: 0.07741643934785679\n",
      "Step 719000, loss: 0.13465549704269505\n",
      "Step 720000, loss: 0.3565957931901248\n",
      "Step 721000, loss: 0.0429460752956511\n",
      "Step 722000, loss: 0.2329497625902295\n",
      "Step 723000, loss: 0.2139533155989593\n",
      "Step 724000, loss: 0.1101687506295566\n",
      "Step 725000, loss: 0.13663556650062675\n",
      "Step 726000, loss: 0.02848395129980054\n",
      "Step 727000, loss: 0.045326666752021995\n",
      "Step 728000, loss: 0.1312629718624812\n",
      "Step 729000, loss: 0.08795394791855687\n",
      "Step 730000, loss: 0.2199014674444011\n",
      "Step 731000, loss: 0.10490271181832941\n",
      "Step 732000, loss: 0.14789770778658567\n",
      "Step 733000, loss: 0.06052438772084497\n",
      "Step 734000, loss: 0.1683523826780147\n",
      "Step 735000, loss: 0.070487421910977\n",
      "Step 736000, loss: 0.058041800384467934\n",
      "Step 737000, loss: 0.16833996425188524\n",
      "Step 738000, loss: 0.09135878727703675\n",
      "Step 739000, loss: 0.2780182440027129\n",
      "Step 740000, loss: 0.13916379999264608\n",
      "Step 741000, loss: 0.018212258775543888\n",
      "Step 742000, loss: 0.16416433253631113\n",
      "Step 743000, loss: 0.4409520528281573\n",
      "Step 744000, loss: 0.16502902494920998\n",
      "Step 745000, loss: 0.14296693045029474\n",
      "Step 746000, loss: 0.033108179117291005\n",
      "Step 747000, loss: 0.09292919190833344\n",
      "Step 748000, loss: 0.10998316815379075\n",
      "Step 749000, loss: 0.20551039396051782\n",
      "Step 750000, loss: 0.035143614341388456\n",
      "Step 751000, loss: 0.09803086535987678\n",
      "Step 752000, loss: 0.03703223615413299\n",
      "Step 753000, loss: 0.07426439679038595\n",
      "Step 754000, loss: 0.10118236606015125\n",
      "Step 755000, loss: 0.17995315753587057\n",
      "Step 756000, loss: 0.017194427126436495\n",
      "Step 757000, loss: 0.10073815597419161\n",
      "Step 758000, loss: 0.1590067957667343\n",
      "Step 759000, loss: 0.05664161102983053\n",
      "Step 760000, loss: 0.021608413054113044\n",
      "Step 761000, loss: 0.08452202929623309\n",
      "Step 762000, loss: 0.212030219924447\n",
      "Step 763000, loss: 0.014598412449486205\n",
      "Step 764000, loss: 0.16247669532668077\n",
      "Step 765000, loss: 0.04032180689118104\n",
      "Step 766000, loss: 0.31080416887198226\n",
      "Step 767000, loss: 0.0032162592055392453\n",
      "Step 768000, loss: 0.025093766709105694\n",
      "Step 769000, loss: 0.02201655884550837\n",
      "Step 770000, loss: 0.09122129016919643\n",
      "Step 771000, loss: 0.02511765486121294\n",
      "Step 772000, loss: 0.07062657697587565\n",
      "Step 773000, loss: 0.07432679448492127\n",
      "Step 774000, loss: 0.07711552059365204\n",
      "Step 775000, loss: 0.08126288623735309\n",
      "Step 776000, loss: 0.014492300779791549\n",
      "Step 777000, loss: 0.3031529933384736\n",
      "Step 778000, loss: 0.08652977417700458\n",
      "Step 779000, loss: 0.19022830930305645\n",
      "Step 780000, loss: 0.14913709868863226\n",
      "Step 781000, loss: 0.11919736049475614\n",
      "Step 782000, loss: 0.07570476899077766\n",
      "Step 783000, loss: -0.03442959772641188\n",
      "Step 784000, loss: 0.11247775796748465\n",
      "Step 785000, loss: -0.043590591472922824\n",
      "Step 786000, loss: 0.015370498829237477\n",
      "Step 787000, loss: 0.07257842914061621\n",
      "Step 788000, loss: 0.06498264264897444\n",
      "Step 789000, loss: -0.015394348718429682\n",
      "Step 790000, loss: 0.19710417214070913\n",
      "Step 791000, loss: 0.23452908071561251\n",
      "Step 792000, loss: 0.031187875039351637\n",
      "Step 793000, loss: 0.14021526798130707\n",
      "Step 794000, loss: 0.17928095551003934\n",
      "Step 795000, loss: 0.10338725998788141\n",
      "Step 796000, loss: 0.0892915910301017\n",
      "Step 797000, loss: 0.03815829677635338\n",
      "Step 798000, loss: 0.050670987102610525\n",
      "Step 799000, loss: 0.04685008805646794\n",
      "Step 800000, loss: 0.005598361355689121\n",
      "Step 801000, loss: 0.15910675394495774\n",
      "Step 802000, loss: 0.06097206578217447\n",
      "Step 803000, loss: -0.014509021339239552\n",
      "Step 804000, loss: 0.08112608176114736\n",
      "Step 805000, loss: 0.045671494066657034\n",
      "Step 806000, loss: 0.11749324429803527\n",
      "Step 807000, loss: -0.041059661866049285\n",
      "Step 808000, loss: 0.17260246406833177\n",
      "Step 809000, loss: 0.21900163159886143\n",
      "Step 810000, loss: -0.06884651621215744\n",
      "Step 811000, loss: 0.041512278455600606\n",
      "Step 812000, loss: 0.0005603585554054007\n",
      "Step 813000, loss: 0.2305768762019179\n",
      "Step 814000, loss: 0.08077226576436078\n",
      "Step 815000, loss: -0.012699150257278235\n",
      "Step 816000, loss: 0.029881351433927193\n",
      "Step 817000, loss: 0.01641675150219817\n",
      "Step 818000, loss: 0.004991339635802433\n",
      "Step 819000, loss: 0.04123262521243305\n",
      "Step 820000, loss: -0.007158899852118338\n",
      "Step 821000, loss: 0.16432235047401628\n",
      "Step 822000, loss: 0.05534786566830007\n",
      "Step 823000, loss: 0.0385426635320182\n",
      "Step 824000, loss: 0.1205049345321313\n",
      "Step 825000, loss: 0.10451443424308672\n",
      "Step 826000, loss: -0.03852893993179896\n",
      "Step 827000, loss: 0.03562539950775681\n",
      "Step 828000, loss: -0.012097948161303065\n",
      "Step 829000, loss: -0.0006410003863129532\n",
      "Step 830000, loss: 0.03452682243095478\n",
      "Step 831000, loss: -0.04926174113665911\n",
      "Step 832000, loss: 0.2183417607753072\n",
      "Step 833000, loss: 0.1346575808315538\n",
      "Step 834000, loss: 0.024638763755880064\n",
      "Step 835000, loss: 0.009345988315297291\n",
      "Step 836000, loss: 0.09542333867929119\n",
      "Step 837000, loss: -0.0672774418907502\n",
      "Step 838000, loss: 0.20446587313211056\n",
      "Step 839000, loss: 0.01354062471605721\n",
      "Step 840000, loss: -0.10271661266614683\n",
      "Step 841000, loss: 0.09512076092441567\n",
      "Step 842000, loss: 0.2021322906484129\n",
      "Step 843000, loss: 0.20012233446512254\n",
      "Step 844000, loss: -0.04504995744887856\n",
      "Step 845000, loss: 0.07590436379321182\n",
      "Step 846000, loss: -0.03621801158733433\n",
      "Step 847000, loss: 0.1495808610128588\n",
      "Step 848000, loss: 0.11072761739377165\n",
      "Step 849000, loss: -0.09109323169037817\n",
      "Step 850000, loss: 0.12069092702244234\n",
      "Step 851000, loss: 0.09758508615836035\n",
      "Step 852000, loss: 0.12369143610537714\n",
      "Step 853000, loss: 0.09209292630123674\n",
      "Step 854000, loss: 0.03941033563297242\n",
      "Step 855000, loss: 0.0430491743258317\n",
      "Step 856000, loss: -0.007908808033782407\n",
      "Step 857000, loss: 0.10236236736434512\n",
      "Step 858000, loss: -0.016902897248975934\n",
      "Step 859000, loss: -0.052010820460389365\n",
      "Step 860000, loss: -0.030272950389422477\n",
      "Step 861000, loss: 0.0454255518924474\n",
      "Step 862000, loss: 0.06136454555444652\n",
      "Step 863000, loss: 0.07249190900050598\n",
      "Step 864000, loss: -0.019904946144815767\n",
      "Step 865000, loss: 0.03567855415769736\n",
      "Step 866000, loss: 0.09209823542000958\n",
      "Step 867000, loss: 0.09012473935559683\n",
      "Step 868000, loss: -0.05388541634044668\n",
      "Step 869000, loss: -0.01665102790950914\n",
      "Step 870000, loss: 0.04790998018800747\n",
      "Step 871000, loss: -0.05966497956798412\n",
      "Step 872000, loss: 0.09817410083135474\n",
      "Step 873000, loss: -0.06853060995182023\n",
      "Step 874000, loss: 0.09381827471475117\n",
      "Step 875000, loss: 0.012209034239611356\n",
      "Step 876000, loss: 0.0017683449270698476\n",
      "Step 877000, loss: -0.03169515903756837\n",
      "Step 878000, loss: 0.030741299471177628\n",
      "Step 879000, loss: -0.022930049850256182\n",
      "Step 880000, loss: 0.01336987367348047\n",
      "Step 881000, loss: -0.010958375173329841\n",
      "Step 882000, loss: 0.10602419294440188\n",
      "Step 883000, loss: -0.0321738812449039\n",
      "Step 884000, loss: 0.03280906733650772\n",
      "Step 885000, loss: 0.09883353346030345\n",
      "Step 886000, loss: 0.026608664672530723\n",
      "Step 887000, loss: 0.11577968166230858\n",
      "Step 888000, loss: -0.0529625752933789\n",
      "Step 889000, loss: -0.008587306877685478\n",
      "Step 890000, loss: -0.057613197217579\n",
      "Step 891000, loss: 0.056893465825363816\n",
      "Step 892000, loss: 0.08300446468454174\n",
      "Step 893000, loss: -0.07979420634475537\n",
      "Step 894000, loss: 0.2166108256893349\n",
      "Step 895000, loss: -0.10135948743287009\n",
      "Step 896000, loss: 0.07775457737850956\n",
      "Step 897000, loss: 0.05045635268685873\n",
      "Step 898000, loss: 0.006392337108438369\n",
      "Step 899000, loss: -0.07977713814523305\n",
      "Step 900000, loss: -0.01750333441814291\n",
      "Step 901000, loss: -0.044580841367074756\n",
      "Step 902000, loss: 0.1695515775543754\n",
      "Step 903000, loss: -0.05863869912060909\n",
      "Step 904000, loss: 0.015354016136203427\n",
      "Step 905000, loss: -0.15824655145255384\n",
      "Step 906000, loss: -0.008078543608542531\n",
      "Step 907000, loss: 0.04053439287848596\n",
      "Step 908000, loss: -0.007716098879114725\n",
      "Step 909000, loss: -0.00749168157830718\n",
      "Step 910000, loss: -0.0433708451561397\n",
      "Step 911000, loss: -0.05020129886132781\n",
      "Step 912000, loss: -0.08807999492413364\n",
      "Step 913000, loss: -0.0667013971683773\n",
      "Step 914000, loss: -0.05018106472771615\n",
      "Step 915000, loss: -0.08246632425696589\n",
      "Step 916000, loss: 0.011757409619633109\n",
      "Step 917000, loss: -0.09990220060350839\n",
      "Step 918000, loss: -0.09764629429613705\n",
      "Step 919000, loss: -0.04515472725685686\n",
      "Step 920000, loss: 0.013693905815987818\n",
      "Step 921000, loss: 0.0346593045971822\n",
      "Step 922000, loss: -0.05111004030634649\n",
      "Step 923000, loss: -0.05645065298234113\n",
      "Step 924000, loss: 0.018857783147876034\n",
      "Step 925000, loss: -0.053177018405593114\n",
      "Step 926000, loss: -0.06245428144468679\n",
      "Step 927000, loss: -0.13452935307484587\n",
      "Step 928000, loss: -0.003304052195744589\n",
      "Step 929000, loss: -0.1247561807591992\n",
      "Step 930000, loss: -0.05949902298906818\n",
      "Step 931000, loss: -0.13623250893712974\n",
      "Step 932000, loss: -0.09076717495423509\n",
      "Step 933000, loss: -0.09538090757734607\n",
      "Step 934000, loss: -0.02316880518331891\n",
      "Step 935000, loss: 0.08189975415688242\n",
      "Step 936000, loss: -0.1054930774662207\n",
      "Step 937000, loss: -0.03556762112193974\n",
      "Step 938000, loss: -0.08965037678935914\n",
      "Step 939000, loss: -0.09811761635565198\n",
      "Step 940000, loss: 0.028241127636400052\n",
      "Step 941000, loss: 0.0072740338744188195\n",
      "Step 942000, loss: -0.14613829189981334\n",
      "Step 943000, loss: -0.055076798130525274\n",
      "Step 944000, loss: -0.13498476995091188\n",
      "Step 945000, loss: -0.06848905116412789\n",
      "Step 946000, loss: -0.03285882844781736\n",
      "Step 947000, loss: -0.08121225307069835\n",
      "Step 948000, loss: -0.06215078062779503\n",
      "Step 949000, loss: -0.02420005032425979\n",
      "Step 950000, loss: 0.016183650958992074\n",
      "Step 951000, loss: -0.1580022799244616\n",
      "Step 952000, loss: -0.14372720772286993\n",
      "Step 953000, loss: 0.03523242889065296\n",
      "Step 954000, loss: -0.10224521756358444\n",
      "Step 955000, loss: -0.09265521529474063\n",
      "Step 956000, loss: 0.00783252231264487\n",
      "Step 957000, loss: -0.13823386621652753\n",
      "Step 958000, loss: -0.059919246517907594\n",
      "Step 959000, loss: -0.040230359429100644\n",
      "Step 960000, loss: -0.028245485039209598\n",
      "Step 961000, loss: -0.15499822921976375\n",
      "Step 962000, loss: 0.002127237781387521\n",
      "Step 963000, loss: -0.23221669842652046\n",
      "Step 964000, loss: -0.05378473554988159\n",
      "Step 965000, loss: -0.06973807775572641\n",
      "Step 966000, loss: -0.15902359849377534\n",
      "Step 967000, loss: 0.0115456962272292\n",
      "Step 968000, loss: -0.1215342151010991\n",
      "Step 969000, loss: -0.1593609805945307\n",
      "Step 970000, loss: -0.053586259941861496\n",
      "Step 971000, loss: -0.10423637420449813\n",
      "Step 972000, loss: -0.1468183494725672\n",
      "Step 973000, loss: -0.13417083990461834\n",
      "Step 974000, loss: -0.07100476930470905\n",
      "Step 975000, loss: -0.07854735142886056\n",
      "Step 976000, loss: -0.028916796520585195\n",
      "Step 977000, loss: -0.10205295263207517\n",
      "Step 978000, loss: -0.14259142513293774\n",
      "Step 979000, loss: -0.11775176888075657\n",
      "Step 980000, loss: -0.15170637622178765\n",
      "Step 981000, loss: -0.13561455736024072\n",
      "Step 982000, loss: -0.09593011119734729\n",
      "Step 983000, loss: -0.04394648690067697\n",
      "Step 984000, loss: -0.12451247469137888\n",
      "Step 985000, loss: -0.11530164685755154\n",
      "Step 986000, loss: -0.1409500213012798\n",
      "Step 987000, loss: -0.13767767698829994\n",
      "Step 988000, loss: -0.048147912439657374\n",
      "Step 989000, loss: -0.10902348312648245\n",
      "Step 990000, loss: -0.07365753754181788\n",
      "Step 991000, loss: -0.06742701657841098\n",
      "Step 992000, loss: -0.04647464891325217\n",
      "Step 993000, loss: -0.15908045949524968\n",
      "Step 994000, loss: 0.05487934920226689\n",
      "Step 995000, loss: -0.1024351660652901\n",
      "Step 996000, loss: -0.12700163430213435\n",
      "Step 997000, loss: 0.017100613019196315\n",
      "Step 998000, loss: -0.01761386230966309\n",
      "Step 999000, loss: -0.06951585742239695\n",
      "Step 1000000, loss: -0.04273255130404141\n",
      "Step 1001000, loss: -0.06129843485890887\n",
      "Step 1002000, loss: 0.07473488767212257\n",
      "Step 1003000, loss: -0.09155992164487543\n",
      "Step 1004000, loss: -0.012291075561588514\n",
      "Step 1005000, loss: -0.1580300432657823\n",
      "Step 1006000, loss: -0.045570071623951665\n",
      "Step 1007000, loss: -0.08216468674968928\n",
      "Step 1008000, loss: -0.11522481851349585\n",
      "Step 1009000, loss: 0.00015371102575954863\n",
      "Step 1010000, loss: -0.20921999333717395\n",
      "Step 1011000, loss: 0.08922836176854616\n",
      "Step 1012000, loss: -0.08465530100621982\n",
      "Step 1013000, loss: -0.1478119488475495\n",
      "Step 1014000, loss: -0.025754305235284845\n",
      "Step 1015000, loss: -0.20137786322156898\n",
      "Step 1016000, loss: -0.11656018921438954\n",
      "Step 1017000, loss: 0.09625473413662985\n",
      "Step 1018000, loss: -0.1475567323024734\n",
      "Step 1019000, loss: -0.1098293323959224\n",
      "Step 1020000, loss: -0.22220422448881436\n",
      "Step 1021000, loss: -0.13023622842680196\n",
      "Step 1022000, loss: -0.07376484332716791\n",
      "Step 1023000, loss: -0.11938906253280583\n",
      "Step 1024000, loss: -0.05065779960656073\n",
      "Step 1025000, loss: -0.09941493405215442\n",
      "Step 1026000, loss: -0.17578430733433925\n",
      "Step 1027000, loss: -0.13862776742479763\n",
      "Step 1028000, loss: -0.1276750059860933\n",
      "Step 1029000, loss: -0.06816788700968937\n",
      "Step 1030000, loss: -0.10766729787137592\n",
      "Step 1031000, loss: -0.03392509510077071\n",
      "Step 1032000, loss: -0.11948504578569555\n",
      "Step 1033000, loss: -0.1010344807576621\n",
      "Step 1034000, loss: -0.1428314139767608\n",
      "Step 1035000, loss: -0.14562570269382558\n",
      "Step 1036000, loss: -0.06340025029596291\n",
      "Step 1037000, loss: -0.08594390888715861\n",
      "Step 1038000, loss: -0.1401557098827325\n",
      "Step 1039000, loss: -0.08666960989974905\n",
      "Step 1040000, loss: -0.17549572085923865\n",
      "Step 1041000, loss: 0.0117761629690649\n",
      "Step 1042000, loss: -0.1450988715242711\n",
      "Step 1043000, loss: -0.07542968098077109\n",
      "Step 1044000, loss: -0.1464473121180199\n",
      "Step 1045000, loss: -0.18790467924752738\n",
      "Step 1046000, loss: -0.07142026413299027\n",
      "Step 1047000, loss: -0.1306907936784555\n",
      "Step 1048000, loss: -0.045511372759472576\n",
      "Step 1049000, loss: -0.17107309093055664\n",
      "Step 1050000, loss: -0.06793136926292209\n",
      "Step 1051000, loss: -0.16317038841417525\n",
      "Step 1052000, loss: -0.2095647387126228\n",
      "Step 1053000, loss: -0.08648012618083158\n",
      "Step 1054000, loss: -0.11194182566832751\n",
      "Step 1055000, loss: -0.15035121668188367\n",
      "Step 1056000, loss: -0.0862090685479343\n",
      "Step 1057000, loss: -0.23816630972176792\n",
      "Step 1058000, loss: -0.03663893391983584\n",
      "Step 1059000, loss: -0.18532100594742223\n",
      "Step 1060000, loss: -0.25229563246120235\n",
      "Step 1061000, loss: -0.1707264332221821\n",
      "Step 1062000, loss: -0.04672185363946482\n",
      "Step 1063000, loss: -0.11878090317233\n",
      "Step 1064000, loss: -0.2089446301604621\n",
      "Step 1065000, loss: -0.06639107283833437\n",
      "Step 1066000, loss: -0.10455204877862707\n",
      "Step 1067000, loss: -0.16411856297741179\n",
      "Step 1068000, loss: -0.11948521949638961\n",
      "Step 1069000, loss: -0.17024804081674666\n",
      "Step 1070000, loss: -0.1837364501711563\n",
      "Step 1071000, loss: -0.15363007679977453\n",
      "Step 1072000, loss: -0.07190458817104808\n",
      "Step 1073000, loss: -0.1516127027918119\n",
      "Step 1074000, loss: -0.1894503458334366\n",
      "Step 1075000, loss: -0.133489191820845\n",
      "Step 1076000, loss: -0.1756195620910148\n",
      "Step 1077000, loss: -0.09705235695603187\n",
      "Step 1078000, loss: -0.12941621874438716\n",
      "Step 1079000, loss: -0.18726068403339013\n",
      "Step 1080000, loss: -0.20765431858377997\n",
      "Step 1081000, loss: -0.118641618667476\n",
      "Step 1082000, loss: -0.16466759972483852\n",
      "Step 1083000, loss: -0.20378126626630547\n",
      "Step 1084000, loss: -0.2524574135456933\n",
      "Step 1085000, loss: -0.25635488234512743\n",
      "Step 1086000, loss: -0.1999434684169246\n",
      "Step 1087000, loss: -0.10495840697945095\n",
      "Step 1088000, loss: -0.10588159540173364\n",
      "Step 1089000, loss: -0.2172732230253314\n",
      "Step 1090000, loss: -0.19914085038821214\n",
      "Step 1091000, loss: -0.09979209401574918\n",
      "Step 1092000, loss: -0.23023137018457054\n",
      "Step 1093000, loss: -0.19182490082085132\n",
      "Step 1094000, loss: -0.18773674340869184\n",
      "Step 1095000, loss: -0.11371501833054935\n",
      "Step 1096000, loss: -0.19328895219127298\n",
      "Step 1097000, loss: -0.22781988936004927\n",
      "Step 1098000, loss: -0.14102441398613155\n",
      "Step 1099000, loss: -0.1568546019871719\n",
      "Step 1100000, loss: -0.1799225999157352\n",
      "Step 1101000, loss: -0.2216095588663302\n",
      "Step 1102000, loss: -0.230464259818953\n",
      "Step 1103000, loss: -0.1769478665604256\n",
      "Step 1104000, loss: -0.1098891968249809\n",
      "Step 1105000, loss: -0.11172459875000641\n",
      "Step 1106000, loss: -0.14820178037753795\n",
      "Step 1107000, loss: -0.2918120710735675\n",
      "Step 1108000, loss: -0.08990809778048424\n",
      "Step 1109000, loss: -0.11911280867963797\n",
      "Step 1110000, loss: -0.2705958399845986\n",
      "Step 1111000, loss: -0.18981908847746673\n",
      "Step 1112000, loss: -0.15490191807114753\n",
      "Step 1113000, loss: -0.25681425463441704\n",
      "Step 1114000, loss: -0.12469072485206562\n",
      "Step 1115000, loss: -0.21909580224810635\n",
      "Step 1116000, loss: -0.08132303651201073\n",
      "Step 1117000, loss: -0.20293348699936178\n",
      "Step 1118000, loss: -0.273949976786389\n",
      "Step 1119000, loss: -0.21602647239877842\n",
      "Step 1120000, loss: -0.049718285287264734\n",
      "Step 1121000, loss: -0.2398961702795932\n",
      "Step 1122000, loss: -0.1923264678779815\n",
      "Step 1123000, loss: -0.027686316872481256\n",
      "Step 1124000, loss: -0.3248738763521542\n",
      "Step 1125000, loss: -0.2131210468452773\n",
      "Step 1126000, loss: -0.18845700480494998\n",
      "Step 1127000, loss: -0.04582130117015913\n",
      "Step 1128000, loss: -0.25460687826853245\n",
      "Step 1129000, loss: -0.1841811402394669\n",
      "Step 1130000, loss: -0.24237335223935225\n",
      "Step 1131000, loss: -0.1304011477670938\n",
      "Step 1132000, loss: -0.20517838570778257\n",
      "Step 1133000, loss: -0.25763243470626185\n",
      "Step 1134000, loss: -0.14598221739020664\n",
      "Step 1135000, loss: -0.13681297749490476\n",
      "Step 1136000, loss: -0.17698772166555865\n",
      "Step 1137000, loss: -0.23917009550426155\n",
      "Step 1138000, loss: -0.13322499669203536\n",
      "Step 1139000, loss: -0.28745206771139054\n",
      "Step 1140000, loss: -0.1783259296255419\n",
      "Step 1141000, loss: -0.18030367089563515\n",
      "Step 1142000, loss: -0.3152300045633456\n",
      "Step 1143000, loss: -0.09468678379245102\n",
      "Step 1144000, loss: -0.002782372389221564\n",
      "Step 1145000, loss: -0.19717975026741624\n",
      "Step 1146000, loss: -0.17734289531024114\n",
      "Step 1147000, loss: -0.19236498819448752\n",
      "Step 1148000, loss: -0.2676575916400179\n",
      "Step 1149000, loss: -0.28125162531752723\n",
      "Step 1150000, loss: -0.2533771545487107\n",
      "Step 1151000, loss: -0.18256131849353552\n",
      "Step 1152000, loss: -0.13564784296753352\n",
      "Step 1153000, loss: -0.12798066526613547\n",
      "Step 1154000, loss: -0.3145492876962671\n",
      "Step 1155000, loss: -0.16228497044858523\n",
      "Step 1156000, loss: -0.10877403894171585\n",
      "Step 1157000, loss: -0.33037467086769173\n",
      "Step 1158000, loss: -0.13714572232063074\n",
      "Step 1159000, loss: -0.2957943183165044\n",
      "Step 1160000, loss: -0.22342816738807597\n",
      "Step 1161000, loss: -0.20405525515938644\n",
      "Step 1162000, loss: -0.3166249733394361\n",
      "Step 1163000, loss: -0.2778802753125783\n",
      "Step 1164000, loss: -0.198447921740124\n",
      "Step 1165000, loss: -0.16790522917773343\n",
      "Step 1166000, loss: -0.33729384143755303\n",
      "Step 1167000, loss: -0.1970566930323839\n",
      "Step 1168000, loss: -0.1408584745307453\n",
      "Step 1169000, loss: -0.19654672442565788\n",
      "Step 1170000, loss: -0.31411065490194595\n",
      "Step 1171000, loss: -0.36321153968549336\n",
      "Step 1172000, loss: -0.09029372889501974\n",
      "Step 1173000, loss: -0.24227016265655402\n",
      "Step 1174000, loss: -0.29973691209964454\n",
      "Step 1175000, loss: -0.14871097474684938\n",
      "Step 1176000, loss: -0.18703625553558959\n",
      "Step 1177000, loss: -0.28034746129474664\n",
      "Step 1178000, loss: -0.2248430072613992\n",
      "Step 1179000, loss: -0.21884965716762236\n",
      "Step 1180000, loss: -0.25870788669894684\n",
      "Step 1181000, loss: -0.26990693836665014\n",
      "Step 1182000, loss: -0.19882730284822173\n",
      "Step 1183000, loss: -0.22868031979177614\n",
      "Step 1184000, loss: -0.23837654690805357\n",
      "Step 1185000, loss: -0.2437972232128377\n",
      "Step 1186000, loss: -0.2172474168802728\n",
      "Step 1187000, loss: -0.18898156072874553\n",
      "Step 1188000, loss: -0.26388678309111857\n",
      "Step 1189000, loss: -0.2718115852327319\n",
      "Step 1190000, loss: -0.29950491837522714\n",
      "Step 1191000, loss: -0.15554247498359472\n",
      "Step 1192000, loss: -0.2520941199193476\n",
      "Step 1193000, loss: -0.23792784389911686\n",
      "Step 1194000, loss: -0.3261028611392248\n",
      "Step 1195000, loss: -0.2771806400141213\n",
      "Step 1196000, loss: -0.2041321337785339\n",
      "Step 1197000, loss: -0.35696961185481635\n",
      "Step 1198000, loss: -0.1866351593293948\n",
      "Step 1199000, loss: -0.312639903455507\n",
      "Step 1200000, loss: -0.15864987174985573\n",
      "Step 1201000, loss: -0.31412562692398205\n",
      "Step 1202000, loss: -0.19957689412805485\n",
      "Step 1203000, loss: -0.16826614250405691\n",
      "Step 1204000, loss: -0.26398844066882154\n",
      "Step 1205000, loss: -0.28832197353879746\n",
      "Step 1206000, loss: -0.31609787796041927\n",
      "Step 1207000, loss: -0.13857114916096908\n",
      "Step 1208000, loss: -0.19305915673752316\n",
      "Step 1209000, loss: -0.37600013714126546\n",
      "Step 1210000, loss: -0.1575641299817944\n",
      "Step 1211000, loss: -0.41866513466392646\n",
      "Step 1212000, loss: -0.21625359231894253\n",
      "Step 1213000, loss: -0.16129297095106448\n",
      "Step 1214000, loss: -0.2951890222838847\n",
      "Step 1215000, loss: -0.2560248826130992\n",
      "Step 1216000, loss: -0.2588232724739937\n",
      "Step 1217000, loss: -0.3393146208175167\n",
      "Step 1218000, loss: -0.2890921886789147\n",
      "Step 1219000, loss: -0.30618217353613\n",
      "Step 1220000, loss: -0.1628238950014347\n",
      "Step 1221000, loss: -0.30490480547508925\n",
      "Step 1222000, loss: -0.06898428666987456\n",
      "Step 1223000, loss: -0.19783803233277286\n",
      "Step 1224000, loss: -0.3763036078783916\n",
      "Step 1225000, loss: -0.17331888411485125\n",
      "Step 1226000, loss: -0.3369216574014281\n",
      "Step 1227000, loss: -0.324687491449411\n",
      "Step 1228000, loss: -0.3437544814867433\n",
      "Step 1229000, loss: -0.28482289874552225\n",
      "Step 1230000, loss: -0.23845165114384145\n",
      "Step 1231000, loss: -0.3233857234803727\n",
      "Step 1232000, loss: -0.3085064128753729\n",
      "Step 1233000, loss: -0.08128099196497351\n",
      "Step 1234000, loss: -0.3391565582107287\n",
      "Step 1235000, loss: -0.3201151757130865\n",
      "Step 1236000, loss: -0.16601504802453565\n",
      "Step 1237000, loss: -0.4299210719243347\n",
      "Step 1238000, loss: -0.22094421344157308\n",
      "Step 1239000, loss: -0.06849464505864307\n",
      "Step 1240000, loss: -0.25571818250836803\n",
      "Step 1241000, loss: -0.24572178996712318\n",
      "Step 1242000, loss: -0.32728134772440537\n",
      "Step 1243000, loss: -0.1965428444966674\n",
      "Step 1244000, loss: -0.4223226730913739\n",
      "Step 1245000, loss: -0.2344305699093966\n",
      "Step 1246000, loss: -0.35446722236817\n",
      "Step 1247000, loss: -0.24103852724540048\n",
      "Step 1248000, loss: -0.2714667475481838\n",
      "Step 1249000, loss: -0.29951816221082117\n",
      "Step 1250000, loss: -0.32648264927882703\n",
      "Step 1251000, loss: -0.2242711481486658\n",
      "Step 1252000, loss: -0.35060265692323445\n",
      "Step 1253000, loss: -0.2594971092861524\n",
      "Step 1254000, loss: -0.2398327649202547\n",
      "Step 1255000, loss: -0.2848280525053851\n",
      "Step 1256000, loss: -0.2901192784665618\n",
      "Step 1257000, loss: -0.35070213353086727\n",
      "Step 1258000, loss: -0.35916760327573866\n",
      "Step 1259000, loss: -0.33527552445884795\n",
      "Step 1260000, loss: -0.3005067315782071\n",
      "Step 1261000, loss: -0.18662348243710586\n",
      "Step 1262000, loss: -0.32422000478376867\n",
      "Step 1263000, loss: -0.28864233148562923\n",
      "Step 1264000, loss: -0.27084039757132994\n",
      "Step 1265000, loss: -0.25797097719850715\n",
      "Step 1266000, loss: -0.39461865090811626\n",
      "Step 1267000, loss: -0.3194376552258618\n",
      "Step 1268000, loss: -0.2538346365379548\n",
      "Step 1269000, loss: -0.24119041787926107\n",
      "Step 1270000, loss: -0.3337087856312864\n",
      "Step 1271000, loss: -0.2820826558161643\n",
      "Step 1272000, loss: -0.4191059028892778\n",
      "Step 1273000, loss: -0.2597480037682108\n",
      "Step 1274000, loss: -0.37691472489549777\n",
      "Step 1275000, loss: -0.2672946689808741\n",
      "Step 1276000, loss: -0.26253513484378344\n",
      "Step 1277000, loss: -0.27773470911054754\n",
      "Step 1278000, loss: -0.2169029208100983\n",
      "Step 1279000, loss: -0.3631338972414378\n",
      "Step 1280000, loss: -0.39839835671079343\n",
      "Step 1281000, loss: -0.16797791034312104\n",
      "Step 1282000, loss: -0.3205894458689727\n",
      "Step 1283000, loss: -0.3315675777934957\n",
      "Step 1284000, loss: -0.3172018672088161\n",
      "Step 1285000, loss: -0.28318693577006343\n",
      "Step 1286000, loss: -0.4279138539048727\n",
      "Step 1287000, loss: -0.2717357053965097\n",
      "Step 1288000, loss: -0.3466003752483521\n",
      "Step 1289000, loss: -0.25958178065903487\n",
      "Step 1290000, loss: -0.2850557846627198\n",
      "Step 1291000, loss: -0.27092929508106317\n",
      "Step 1292000, loss: -0.39010948235100656\n",
      "Step 1293000, loss: -0.24156400107993978\n",
      "Step 1294000, loss: -0.26184126022655985\n",
      "Step 1295000, loss: -0.3193411461795913\n",
      "Step 1296000, loss: -0.3453219382085372\n",
      "Step 1297000, loss: -0.4207812989402446\n",
      "Step 1298000, loss: -0.30802902831466056\n",
      "Step 1299000, loss: -0.2754546413918724\n",
      "Step 1300000, loss: -0.2645814634314738\n",
      "Step 1301000, loss: -0.20604572238889524\n",
      "Step 1302000, loss: -0.2407897998946719\n",
      "Step 1303000, loss: -0.28949369353661314\n",
      "Step 1304000, loss: -0.28494370927475393\n",
      "Step 1305000, loss: -0.31046752555971036\n",
      "Step 1306000, loss: -0.30074229364143684\n",
      "Step 1307000, loss: -0.3226516532485839\n",
      "Step 1308000, loss: -0.42038094930676745\n",
      "Step 1309000, loss: -0.25903279909002597\n",
      "Step 1310000, loss: -0.34413841309491544\n",
      "Step 1311000, loss: -0.3746357866146136\n",
      "Step 1312000, loss: -0.4052090908510145\n",
      "Step 1313000, loss: -0.38915941917139574\n",
      "Step 1314000, loss: -0.3509322884411085\n",
      "Step 1315000, loss: -0.2773926148682367\n",
      "Step 1316000, loss: -0.30514203430195674\n",
      "Step 1317000, loss: -0.4640354315184522\n",
      "Step 1318000, loss: -0.18806636204887764\n",
      "Step 1319000, loss: -0.2548063962489832\n",
      "Step 1320000, loss: -0.3183912284622202\n",
      "Step 1321000, loss: -0.3249379646485904\n",
      "Step 1322000, loss: -0.36047281166362516\n",
      "Step 1323000, loss: -0.3626161419543787\n",
      "Step 1324000, loss: -0.2881828598503489\n",
      "Step 1325000, loss: -0.31474681721720843\n",
      "Step 1326000, loss: -0.34186287228456425\n",
      "Step 1327000, loss: -0.31994212087523194\n",
      "Step 1328000, loss: -0.32390783644356996\n",
      "Step 1329000, loss: -0.2829116020317888\n",
      "Step 1330000, loss: -0.28099878271666967\n",
      "Step 1331000, loss: -0.4062727229917291\n",
      "Step 1332000, loss: -0.24025967856193892\n",
      "Step 1333000, loss: -0.3622717179202737\n",
      "Step 1334000, loss: -0.3546430299738422\n",
      "Step 1335000, loss: -0.30803373921616\n",
      "Step 1336000, loss: -0.4269017651955364\n",
      "Step 1337000, loss: -0.26963319090852744\n",
      "Step 1338000, loss: -0.38157983481971314\n",
      "Step 1339000, loss: -0.4186562295287149\n",
      "Step 1340000, loss: -0.20770169545343378\n",
      "Step 1341000, loss: -0.36520608453825115\n",
      "Step 1342000, loss: -0.33477866468718276\n",
      "Step 1343000, loss: -0.3673089485202217\n",
      "Step 1344000, loss: -0.37935427179478576\n",
      "Step 1345000, loss: -0.3159259088635445\n",
      "Step 1346000, loss: -0.32316659000638176\n",
      "Step 1347000, loss: -0.19304376976797358\n",
      "Step 1348000, loss: -0.27523617240739984\n",
      "Step 1349000, loss: -0.3332355911533814\n",
      "Step 1350000, loss: -0.2742537317488168\n",
      "Step 1351000, loss: -0.37195237015315796\n",
      "Step 1352000, loss: -0.4338963689240627\n",
      "Step 1353000, loss: -0.4044937865849352\n",
      "Step 1354000, loss: -0.3548760893389117\n",
      "Step 1355000, loss: -0.31824682627328366\n",
      "Step 1356000, loss: -0.3000771745988168\n",
      "Step 1357000, loss: -0.17566699489241\n",
      "Step 1358000, loss: -0.07430931510240771\n",
      "Step 1359000, loss: -0.4533707258503418\n",
      "Step 1360000, loss: -0.42326002759626136\n",
      "Step 1361000, loss: -0.40272416690678803\n",
      "Step 1362000, loss: -0.34014945806656033\n",
      "Step 1363000, loss: -0.36500806231668687\n",
      "Step 1364000, loss: -0.35291275452077386\n",
      "Step 1365000, loss: -0.2210585500812158\n",
      "Step 1366000, loss: -0.14586864302284083\n",
      "Step 1367000, loss: -0.4094311523216311\n",
      "Step 1368000, loss: -0.3428547945175087\n",
      "Step 1369000, loss: -0.4504990709495032\n",
      "Step 1370000, loss: -0.332573092632927\n",
      "Step 1371000, loss: -0.42963999351742677\n",
      "Step 1372000, loss: -0.38322183051946923\n",
      "Step 1373000, loss: -0.4294629215844907\n",
      "Step 1374000, loss: -0.24204757735150634\n",
      "Step 1375000, loss: -0.42167208776826737\n",
      "Step 1376000, loss: -0.2506543419791269\n",
      "Step 1377000, loss: -0.25910751560283823\n",
      "Step 1378000, loss: -0.3969138727833124\n",
      "Step 1379000, loss: -0.3106310610861983\n",
      "Step 1380000, loss: -0.3464607041537529\n",
      "Step 1381000, loss: -0.37081835968012455\n",
      "Step 1382000, loss: -0.453809130549198\n",
      "Step 1383000, loss: -0.2182067152284435\n",
      "Step 1384000, loss: -0.4384326164478698\n",
      "Step 1385000, loss: -0.448490555819124\n",
      "Step 1386000, loss: -0.36811742011323806\n",
      "Step 1387000, loss: -0.32937358196900457\n",
      "Step 1388000, loss: -0.34019808261629075\n",
      "Step 1389000, loss: -0.43512267509911906\n",
      "Step 1390000, loss: -0.20785079597019648\n",
      "Step 1391000, loss: -0.459646160860575\n",
      "Step 1392000, loss: -0.3452953514010878\n",
      "Step 1393000, loss: -0.41633704482464234\n",
      "Step 1394000, loss: -0.40037582340883093\n",
      "Step 1395000, loss: -0.33072706941049546\n",
      "Step 1396000, loss: -0.42279571433867386\n",
      "Step 1397000, loss: -0.32461165807233194\n",
      "Step 1398000, loss: -0.2691086903460091\n",
      "Step 1399000, loss: -0.3022870834511705\n",
      "Step 1400000, loss: -0.34332804216066143\n",
      "Step 1401000, loss: -0.3282234022275079\n",
      "Step 1402000, loss: -0.47548405010206624\n",
      "Step 1403000, loss: -0.29621244214486797\n",
      "Step 1404000, loss: -0.34904775018268264\n",
      "Step 1405000, loss: -0.39437753860047087\n",
      "Step 1406000, loss: -0.3599223679068964\n",
      "Step 1407000, loss: -0.3627211996533442\n",
      "Step 1408000, loss: -0.29747219844791106\n",
      "Step 1409000, loss: -0.44210027781491223\n",
      "Step 1410000, loss: -0.3990339846871357\n",
      "Step 1411000, loss: -0.42456548823975027\n",
      "Step 1412000, loss: -0.3599717955396045\n",
      "Step 1413000, loss: -0.3902406463243242\n",
      "Step 1414000, loss: -0.35443777490756473\n",
      "Step 1415000, loss: -0.2866363693875028\n",
      "Step 1416000, loss: -0.42129475316987375\n",
      "Step 1417000, loss: -0.434172542323824\n",
      "Step 1418000, loss: -0.39473940678383224\n",
      "Step 1419000, loss: -0.2618048961462919\n",
      "Step 1420000, loss: -0.44264416634949155\n",
      "Step 1421000, loss: -0.23893333605886438\n",
      "Step 1422000, loss: -0.5043621388226748\n",
      "Step 1423000, loss: -0.3945953191051085\n",
      "Step 1424000, loss: -0.399908116209408\n",
      "Step 1425000, loss: -0.4183633299616049\n",
      "Step 1426000, loss: -0.4160739536336623\n",
      "Step 1427000, loss: -0.31715846697092637\n",
      "Step 1428000, loss: -0.46730100180837325\n",
      "Step 1429000, loss: -0.36988968818378637\n",
      "Step 1430000, loss: -0.42629259735462255\n",
      "Step 1431000, loss: -0.32438762579485775\n",
      "Step 1432000, loss: -0.4621834183352039\n",
      "Step 1433000, loss: -0.31918355777696705\n",
      "Step 1434000, loss: -0.30908250830392353\n",
      "Step 1435000, loss: -0.3051946875953581\n",
      "Step 1436000, loss: -0.38586582482978704\n",
      "Step 1437000, loss: -0.4650732121056935\n",
      "Step 1438000, loss: -0.24031115011318616\n",
      "Step 1439000, loss: -0.3335714239038061\n",
      "Step 1440000, loss: -0.46122787513490765\n",
      "Step 1441000, loss: -0.2546800712258555\n",
      "Step 1442000, loss: -0.4036410184503766\n",
      "Step 1443000, loss: -0.3999077256862074\n",
      "Step 1444000, loss: -0.4665324315270409\n",
      "Step 1445000, loss: -0.39780695973103863\n",
      "Step 1446000, loss: -0.39480024344834963\n",
      "Step 1447000, loss: -0.42523054391291226\n",
      "Step 1448000, loss: -0.30841493429103867\n",
      "Step 1449000, loss: -0.18630252745491452\n",
      "Step 1450000, loss: -0.41794897489761934\n",
      "Step 1451000, loss: -0.3681010423232801\n",
      "Step 1452000, loss: -0.1488027663968969\n",
      "Step 1453000, loss: -0.4049699656912126\n",
      "Step 1454000, loss: -0.3909365440252004\n",
      "Step 1455000, loss: -0.37291940245497973\n",
      "Step 1456000, loss: -0.4110790924461908\n",
      "Step 1457000, loss: -0.3992920913059497\n",
      "Step 1458000, loss: -0.33784212084673343\n",
      "Step 1459000, loss: -0.4060855691607576\n",
      "Step 1460000, loss: -0.2725774603523314\n",
      "Step 1461000, loss: -0.4778859790916904\n",
      "Step 1462000, loss: -0.4098811608622782\n",
      "Step 1463000, loss: -0.40335548088745415\n",
      "Step 1464000, loss: -0.43479128217883406\n",
      "Step 1465000, loss: -0.3089186053674202\n",
      "Step 1466000, loss: -0.3960720568718389\n",
      "Step 1467000, loss: -0.44504787398781626\n",
      "Step 1468000, loss: -0.34669827960501426\n",
      "Step 1469000, loss: -0.3986324643190019\n",
      "Step 1470000, loss: -0.4285114984184038\n",
      "Step 1471000, loss: -0.3691561580214184\n",
      "Step 1472000, loss: -0.40590378949581646\n",
      "Step 1473000, loss: -0.3301804620521434\n",
      "Step 1474000, loss: -0.34841562471759974\n",
      "Step 1475000, loss: -0.4452812160523608\n",
      "Step 1476000, loss: -0.19961576353904092\n",
      "Step 1477000, loss: -0.35384140821394977\n",
      "Step 1478000, loss: -0.44332395732053553\n",
      "Step 1479000, loss: -0.3904899729747558\n",
      "Step 1480000, loss: -0.45918252701958406\n",
      "Step 1481000, loss: -0.26749021921190436\n",
      "Step 1482000, loss: -0.27504201509593984\n",
      "Step 1483000, loss: -0.46663303711556364\n",
      "Step 1484000, loss: -0.4422978726706933\n",
      "Step 1485000, loss: -0.17177873889380135\n",
      "Step 1486000, loss: -0.2927244849173585\n",
      "Step 1487000, loss: -0.42995282827410847\n",
      "Step 1488000, loss: -0.2788020536655095\n",
      "Step 1489000, loss: -0.16370449619298597\n",
      "Step 1490000, loss: -0.29420419287739785\n",
      "Step 1491000, loss: -0.5076945754981134\n",
      "Step 1492000, loss: -0.3530441639432684\n",
      "Step 1493000, loss: -0.4314708608366782\n",
      "Step 1494000, loss: -0.2603393675305415\n",
      "Step 1495000, loss: -0.3704442020095885\n",
      "Step 1496000, loss: -0.3475702342640143\n",
      "Step 1497000, loss: -0.40842796183051544\n",
      "Step 1498000, loss: -0.3631943330685608\n",
      "Step 1499000, loss: -0.42247956114821134\n",
      "Step 1500000, loss: -0.32072853729725465\n",
      "Step 1501000, loss: -0.5209789175091719\n",
      "Step 1502000, loss: -0.3804708932158537\n",
      "Step 1503000, loss: -0.3550555249203462\n",
      "Step 1504000, loss: -0.4488817966248607\n",
      "Step 1505000, loss: -0.18559428750223014\n",
      "Step 1506000, loss: -0.44237511842604726\n",
      "Step 1507000, loss: -0.44220902024698444\n",
      "Step 1508000, loss: -0.4131364759069402\n",
      "Step 1509000, loss: -0.5145857689995319\n",
      "Step 1510000, loss: -0.1646244186177064\n",
      "Step 1511000, loss: -0.41573986574728045\n",
      "Step 1512000, loss: -0.4743701818569971\n",
      "Step 1513000, loss: -0.293088682651578\n",
      "Step 1514000, loss: -0.5586095774624263\n",
      "Step 1515000, loss: -0.3272162890591144\n",
      "Step 1516000, loss: -0.4018338966614101\n",
      "Step 1517000, loss: -0.3813066103848687\n",
      "Step 1518000, loss: -0.26128796182456426\n",
      "Step 1519000, loss: -0.3551131617059582\n",
      "Step 1520000, loss: -0.41532393828884234\n",
      "Step 1521000, loss: -0.5182140791791026\n",
      "Step 1522000, loss: -0.40923862816719336\n",
      "Step 1523000, loss: -0.47963805281274835\n",
      "Step 1524000, loss: -0.36768577304114297\n",
      "Step 1525000, loss: -0.4283713227524422\n",
      "Step 1526000, loss: -0.4784990860798571\n",
      "Step 1527000, loss: -0.3463400208707899\n",
      "Step 1528000, loss: -0.3314993286737008\n",
      "Step 1529000, loss: -0.4445726064056507\n",
      "Step 1530000, loss: -0.44314628705196085\n",
      "Step 1531000, loss: -0.22618875665139057\n",
      "Step 1532000, loss: -0.41821913041872905\n",
      "Step 1533000, loss: -0.5176886205038754\n",
      "Step 1534000, loss: -0.4491847958874423\n",
      "Step 1535000, loss: -0.26548052864003696\n",
      "Step 1536000, loss: -0.3578896839936497\n",
      "Step 1537000, loss: -0.4649504347078037\n",
      "Step 1538000, loss: -0.3590122487297704\n",
      "Step 1539000, loss: -0.515830727567256\n",
      "Step 1540000, loss: -0.3186915396321565\n",
      "Step 1541000, loss: -0.4885235165723134\n",
      "Step 1542000, loss: -0.39541218648734505\n",
      "Step 1543000, loss: -0.44209889085637405\n",
      "Step 1544000, loss: -0.42856189171387815\n",
      "Step 1545000, loss: -0.39811008872726233\n",
      "Step 1546000, loss: -0.5205977449251804\n",
      "Step 1547000, loss: -0.4001373366264161\n",
      "Step 1548000, loss: -0.40393235740007366\n",
      "Step 1549000, loss: -0.4817364349548188\n",
      "Step 1550000, loss: -0.39896703574899584\n",
      "Step 1551000, loss: -0.5096364794803084\n",
      "Step 1552000, loss: -0.3996398736144474\n",
      "Step 1553000, loss: -0.43379618757287974\n",
      "Step 1554000, loss: -0.5276566573809832\n",
      "Step 1555000, loss: -0.3087240469910612\n",
      "Step 1556000, loss: -0.45570317133981736\n",
      "Step 1557000, loss: -0.4610571479568025\n",
      "Step 1558000, loss: -0.42591454232901743\n",
      "Step 1559000, loss: -0.469801054533571\n",
      "Step 1560000, loss: -0.5215638334848918\n",
      "Step 1561000, loss: -0.40531324614735786\n",
      "Step 1562000, loss: -0.5181186027439544\n",
      "Step 1563000, loss: -0.4792945691479836\n",
      "Step 1564000, loss: -0.46422398181492464\n",
      "Step 1565000, loss: -0.5742234602652024\n",
      "Step 1566000, loss: -0.44298129458108454\n",
      "Step 1567000, loss: -0.493694459104212\n",
      "Step 1568000, loss: -0.3179199098169338\n",
      "Step 1569000, loss: -0.4696426236424595\n",
      "Step 1570000, loss: -0.28275948245287874\n",
      "Step 1571000, loss: -0.5012931424979761\n",
      "Step 1572000, loss: -0.6017416246063076\n",
      "Step 1573000, loss: -0.4382603788321503\n",
      "Step 1574000, loss: -0.39385476042330264\n",
      "Step 1575000, loss: -0.4110391319910996\n",
      "Step 1576000, loss: -0.41808187229419125\n",
      "Step 1577000, loss: -0.5080395239857025\n",
      "Step 1578000, loss: -0.424262664653128\n",
      "Step 1579000, loss: -0.41239533659024163\n",
      "Step 1580000, loss: -0.4093055790117942\n",
      "Step 1581000, loss: -0.6173986530649709\n",
      "Step 1582000, loss: -0.4151593779746909\n",
      "Step 1583000, loss: -0.35907674536143896\n",
      "Step 1584000, loss: -0.49707552209193817\n",
      "Step 1585000, loss: -0.4180153156388551\n",
      "Step 1586000, loss: -0.5594303373929579\n",
      "Step 1587000, loss: -0.5608289140430279\n",
      "Step 1588000, loss: -0.4782508125049062\n",
      "Step 1589000, loss: -0.5204026375237881\n",
      "Step 1590000, loss: -0.32924173746129964\n",
      "Step 1591000, loss: -0.5308322271663929\n",
      "Step 1592000, loss: -0.44288285037351305\n",
      "Step 1593000, loss: -0.46508491117721135\n",
      "Step 1594000, loss: -0.29288362830778353\n",
      "Step 1595000, loss: -0.5527815490220528\n",
      "Step 1596000, loss: -0.4242931389643345\n",
      "Step 1597000, loss: -0.3196095362259075\n",
      "Step 1598000, loss: -0.5175584750901325\n",
      "Step 1599000, loss: -0.48361107043060475\n",
      "Step 1600000, loss: -0.5364358983311686\n",
      "Step 1601000, loss: -0.49187987577263265\n",
      "Step 1602000, loss: -0.35868167961540165\n",
      "Step 1603000, loss: -0.4814890206051059\n",
      "Step 1604000, loss: -0.25696926353813615\n",
      "Step 1605000, loss: -0.42588594995997847\n",
      "Step 1606000, loss: -0.4267072623544373\n",
      "Step 1607000, loss: -0.5238830123043153\n",
      "Step 1608000, loss: -0.5287115836061712\n",
      "Step 1609000, loss: -0.3933720949070994\n",
      "Step 1610000, loss: -0.46770037182664964\n",
      "Step 1611000, loss: -0.6088246133469802\n",
      "Step 1612000, loss: -0.4075532491512713\n",
      "Step 1613000, loss: -0.5640143937181565\n",
      "Step 1614000, loss: -0.5467371793157072\n",
      "Step 1615000, loss: -0.3835837224292336\n",
      "Step 1616000, loss: -0.4793571162133012\n",
      "Step 1617000, loss: -0.5286802709558979\n",
      "Step 1618000, loss: -0.49173667619377376\n",
      "Step 1619000, loss: -0.5132560853646136\n",
      "Step 1620000, loss: -0.4646524732040707\n",
      "Step 1621000, loss: -0.3890533310687169\n",
      "Step 1622000, loss: -0.40472369781869927\n",
      "Step 1623000, loss: -0.45493819580296985\n",
      "Step 1624000, loss: -0.5684571679793152\n",
      "Step 1625000, loss: -0.5354556602159574\n",
      "Step 1626000, loss: -0.40296006566355935\n",
      "Step 1627000, loss: -0.5140709298205911\n",
      "Step 1628000, loss: -0.48504729901786775\n",
      "Step 1629000, loss: -0.5068053164125886\n",
      "Step 1630000, loss: -0.4728221922702069\n",
      "Step 1631000, loss: -0.5358607483739033\n",
      "Step 1632000, loss: -0.37254774298414123\n",
      "Step 1633000, loss: -0.5746239886325784\n",
      "Step 1634000, loss: -0.4748086670562043\n",
      "Step 1635000, loss: -0.40870175540226045\n",
      "Step 1636000, loss: -0.4112737951740855\n",
      "Step 1637000, loss: -0.5594115726544987\n",
      "Step 1638000, loss: -0.3735456784842536\n",
      "Step 1639000, loss: -0.5550680855808314\n",
      "Step 1640000, loss: -0.44180849385960025\n",
      "Step 1641000, loss: -0.3891214722779114\n",
      "Step 1642000, loss: -0.6075816819227766\n",
      "Step 1643000, loss: -0.31133730082563127\n",
      "Step 1644000, loss: -0.6131751357516623\n",
      "Step 1645000, loss: -0.5727595403017476\n",
      "Step 1646000, loss: -0.4476895325026708\n",
      "Step 1647000, loss: -0.42790020102943527\n",
      "Step 1648000, loss: -0.5251143572899164\n",
      "Step 1649000, loss: -0.3123105875843321\n",
      "Step 1650000, loss: -0.4739992231709475\n",
      "Step 1651000, loss: -0.5388848267728463\n",
      "Step 1652000, loss: -0.5587684387028566\n",
      "Step 1653000, loss: -0.43826561483717524\n",
      "Step 1654000, loss: -0.5284577469742507\n",
      "Step 1655000, loss: -0.39635034174844624\n",
      "Step 1656000, loss: -0.3863079825965688\n",
      "Step 1657000, loss: -0.45876575593929736\n",
      "Step 1658000, loss: -0.5766483053595294\n",
      "Step 1659000, loss: -0.6566183002814651\n",
      "Step 1660000, loss: -0.3905698654813459\n",
      "Step 1661000, loss: -0.5792177864640252\n",
      "Step 1662000, loss: -0.5401213250636356\n",
      "Step 1663000, loss: -0.6034648323251749\n",
      "Step 1664000, loss: -0.3818255656013498\n",
      "Step 1665000, loss: -0.5874501190686424\n",
      "Step 1666000, loss: -0.5085215377921705\n",
      "Step 1667000, loss: -0.5430802229701076\n",
      "Step 1668000, loss: -0.4944664407297969\n",
      "Step 1669000, loss: -0.43576781523774843\n",
      "Step 1670000, loss: -0.4291128668613965\n",
      "Step 1671000, loss: -0.5354803937781835\n",
      "Step 1672000, loss: -0.5927578461952507\n",
      "Step 1673000, loss: -0.5384917870836798\n",
      "Step 1674000, loss: -0.44848495902866126\n",
      "Step 1675000, loss: -0.559286529617093\n",
      "Step 1676000, loss: -0.4824320687416475\n",
      "Step 1677000, loss: -0.35897073180181904\n",
      "Step 1678000, loss: -0.5500891837729142\n",
      "Step 1679000, loss: -0.617075963093208\n",
      "Step 1680000, loss: -0.49874964603720584\n",
      "Step 1681000, loss: -0.5812929185770918\n",
      "Step 1682000, loss: -0.5715262419503415\n",
      "Step 1683000, loss: -0.49353395820688456\n",
      "Step 1684000, loss: -0.5543349656074424\n",
      "Step 1685000, loss: -0.5076391218735371\n",
      "Step 1686000, loss: -0.5392264383547009\n",
      "Step 1687000, loss: -0.5246702947682352\n",
      "Step 1688000, loss: -0.47947686394909395\n",
      "Step 1689000, loss: -0.4407320660459809\n",
      "Step 1690000, loss: -0.5445391680439934\n",
      "Step 1691000, loss: -0.5303193512029247\n",
      "Step 1692000, loss: -0.5375892228567973\n",
      "Step 1693000, loss: -0.34909415594121673\n",
      "Step 1694000, loss: -0.5830603926274925\n",
      "Step 1695000, loss: -0.47687841601676334\n",
      "Step 1696000, loss: -0.521898298892047\n",
      "Step 1697000, loss: -0.5605566598454025\n",
      "Step 1698000, loss: -0.5938685470344499\n",
      "Step 1699000, loss: -0.45766273259170703\n",
      "Step 1700000, loss: -0.45007241143658755\n",
      "Step 1701000, loss: -0.507376035306399\n",
      "Step 1702000, loss: -0.5018505275449715\n",
      "Step 1703000, loss: -0.4780360208459315\n",
      "Step 1704000, loss: -0.5631904513308545\n",
      "Step 1705000, loss: -0.336201559239591\n",
      "Step 1706000, loss: -0.6798518554044422\n",
      "Step 1707000, loss: -0.44135399302956646\n",
      "Step 1708000, loss: -0.5273616068845149\n",
      "Step 1709000, loss: -0.5361694052959792\n",
      "Step 1710000, loss: -0.5519052199637517\n",
      "Step 1711000, loss: -0.41588395898917224\n",
      "Step 1712000, loss: -0.6614409200649243\n",
      "Step 1713000, loss: -0.5206298922240967\n",
      "Step 1714000, loss: -0.5113362886253744\n",
      "Step 1715000, loss: -0.4603352608892019\n",
      "Step 1716000, loss: -0.4313496296494268\n",
      "Step 1717000, loss: -0.4753674798794091\n",
      "Step 1718000, loss: -0.5348262896628585\n",
      "Step 1719000, loss: -0.36839503709087146\n",
      "Step 1720000, loss: -0.3572278019565274\n",
      "Step 1721000, loss: -0.4703134194582235\n",
      "Step 1722000, loss: -0.5807669557959889\n",
      "Step 1723000, loss: -0.4544444478287478\n",
      "Step 1724000, loss: -0.5045399113488384\n",
      "Step 1725000, loss: -0.48372203834731226\n",
      "Step 1726000, loss: -0.5988467343049124\n",
      "Step 1727000, loss: -0.5800588108140801\n",
      "Step 1728000, loss: -0.43903485567122696\n",
      "Step 1729000, loss: -0.4207354615778895\n",
      "Step 1730000, loss: -0.5860851592024555\n",
      "Step 1731000, loss: -0.4692500134683214\n",
      "Step 1732000, loss: -0.584204477331019\n",
      "Step 1733000, loss: -0.5563222582727904\n",
      "Step 1734000, loss: -0.48208405856086756\n",
      "Step 1735000, loss: -0.4452772542099701\n",
      "Step 1736000, loss: -0.5475340869475039\n",
      "Step 1737000, loss: -0.421016986831557\n",
      "Step 1738000, loss: -0.6499970085071399\n",
      "Step 1739000, loss: -0.5012987807695172\n",
      "Step 1740000, loss: -0.5262794646681287\n",
      "Step 1741000, loss: -0.5627401231515687\n",
      "Step 1742000, loss: -0.41555489606407353\n",
      "Step 1743000, loss: -0.4348579433420673\n",
      "Step 1744000, loss: -0.5866451591774822\n",
      "Step 1745000, loss: -0.5194621988220606\n",
      "Step 1746000, loss: -0.41881184742227195\n",
      "Step 1747000, loss: -0.5578929013442248\n",
      "Step 1748000, loss: -0.40962043167860246\n",
      "Step 1749000, loss: -0.521607074869331\n",
      "Step 1750000, loss: -0.5267860838584602\n",
      "Step 1751000, loss: -0.4632976928646676\n",
      "Step 1752000, loss: -0.6209006458628573\n",
      "Step 1753000, loss: -0.5205300302812429\n",
      "Step 1754000, loss: -0.6046269563883834\n",
      "Step 1755000, loss: -0.47686692480929194\n",
      "Step 1756000, loss: -0.44166670968569816\n",
      "Step 1757000, loss: -0.44953184903098736\n",
      "Step 1758000, loss: -0.5212143798263278\n",
      "Step 1759000, loss: -0.5934658625684679\n",
      "Step 1760000, loss: -0.44918337575462647\n",
      "Step 1761000, loss: -0.6168031177402008\n",
      "Step 1762000, loss: -0.5566153540534724\n",
      "Step 1763000, loss: -0.577290081653453\n",
      "Step 1764000, loss: -0.5363508921884932\n",
      "Step 1765000, loss: -0.575681882957957\n",
      "Step 1766000, loss: -0.4424290180280805\n",
      "Step 1767000, loss: -0.43778100974619155\n",
      "Step 1768000, loss: -0.5061361823066254\n",
      "Step 1769000, loss: -0.5645165721296799\n",
      "Step 1770000, loss: -0.6113056089649909\n",
      "Step 1771000, loss: -0.5969038733650813\n",
      "Step 1772000, loss: -0.5694766105846502\n",
      "Step 1773000, loss: -0.5513633483116283\n",
      "Step 1774000, loss: -0.6404046625792981\n",
      "Step 1775000, loss: -0.5313588597977942\n",
      "Step 1776000, loss: -0.5361067394262645\n",
      "Step 1777000, loss: -0.46060476656781973\n",
      "Step 1778000, loss: -0.5514924472603016\n",
      "Step 1779000, loss: -0.6073878671372076\n",
      "Step 1780000, loss: -0.5972796861233656\n",
      "Step 1781000, loss: -0.45450325373956\n",
      "Step 1782000, loss: -0.5634981158267474\n",
      "Step 1783000, loss: -0.6473790953750722\n",
      "Step 1784000, loss: -0.49488051245734094\n",
      "Step 1785000, loss: -0.5947863512365148\n",
      "Step 1786000, loss: -0.5581993252635002\n",
      "Step 1787000, loss: -0.4761792041505687\n",
      "Step 1788000, loss: -0.697106928106863\n",
      "Step 1789000, loss: -0.49047955463145626\n",
      "Step 1790000, loss: -0.5450524392003717\n",
      "Step 1791000, loss: -0.42408785362687196\n",
      "Step 1792000, loss: -0.6050141033837572\n",
      "Step 1793000, loss: -0.5345997173575161\n",
      "Step 1794000, loss: -0.6172827855845681\n",
      "Step 1795000, loss: -0.6277758153697359\n",
      "Step 1796000, loss: -0.587281471727998\n",
      "Step 1797000, loss: -0.5539179953136482\n",
      "Step 1798000, loss: -0.6337825662330724\n",
      "Step 1799000, loss: -0.447310035422619\n",
      "Step 1800000, loss: -0.6029761325572618\n",
      "Step 1801000, loss: -0.5435704467841425\n",
      "Step 1802000, loss: -0.5458281726416899\n",
      "Step 1803000, loss: -0.5872892137598101\n",
      "Step 1804000, loss: -0.5730162333403714\n",
      "Step 1805000, loss: -0.43507952290936375\n",
      "Step 1806000, loss: -0.6177783821108751\n",
      "Step 1807000, loss: -0.598795301042017\n",
      "Step 1808000, loss: -0.6730279483008198\n",
      "Step 1809000, loss: -0.4450152962531429\n",
      "Step 1810000, loss: -0.5344431958318601\n",
      "Step 1811000, loss: -0.6219880598435411\n",
      "Step 1812000, loss: -0.5103631699196994\n",
      "Step 1813000, loss: -0.6282383343803231\n",
      "Step 1814000, loss: -0.5779469557185657\n",
      "Step 1815000, loss: -0.44459119958337395\n",
      "Step 1816000, loss: -0.48348390398547053\n",
      "Step 1817000, loss: -0.5017933917795017\n",
      "Step 1818000, loss: -0.6072326547289267\n",
      "Step 1819000, loss: -0.5837176591555472\n",
      "Step 1820000, loss: -0.6510215820390731\n",
      "Step 1821000, loss: -0.5531570917974459\n",
      "Step 1822000, loss: -0.626994233624544\n",
      "Step 1823000, loss: -0.4360314472778991\n",
      "Step 1824000, loss: -0.6159450099074311\n",
      "Step 1825000, loss: -0.6490089221901726\n",
      "Step 1826000, loss: -0.5537387722936401\n",
      "Step 1827000, loss: -0.5339082979228115\n",
      "Step 1828000, loss: -0.5533852521913067\n",
      "Step 1829000, loss: -0.5859842510822928\n",
      "Step 1830000, loss: -0.4927334549789084\n",
      "Step 1831000, loss: -0.6259556283312849\n",
      "Step 1832000, loss: -0.5740896818144247\n",
      "Step 1833000, loss: -0.5907876561873127\n",
      "Step 1834000, loss: -0.5808482037076028\n",
      "Step 1835000, loss: -0.46456449551973494\n",
      "Step 1836000, loss: -0.6478395169209689\n",
      "Step 1837000, loss: -0.6248879693106283\n",
      "Step 1838000, loss: -0.5860101516199065\n",
      "Step 1839000, loss: -0.5330353480787017\n",
      "Step 1840000, loss: -0.6099355683219619\n",
      "Step 1841000, loss: -0.6146718061380089\n",
      "Step 1842000, loss: -0.4926737901291344\n",
      "Step 1843000, loss: -0.5705991430896101\n",
      "Step 1844000, loss: -0.5600348653656693\n",
      "Step 1845000, loss: -0.5116294074184261\n",
      "Step 1846000, loss: -0.6188105390374549\n",
      "Step 1847000, loss: -0.7521723227538168\n",
      "Step 1848000, loss: -0.5138454186839517\n",
      "Step 1849000, loss: -0.5385194924228708\n",
      "Step 1850000, loss: -0.5988175925009418\n",
      "Step 1851000, loss: -0.5340208747067663\n",
      "Step 1852000, loss: -0.554159823582042\n",
      "Step 1853000, loss: -0.48626100530027905\n",
      "Step 1854000, loss: -0.6624529948793352\n",
      "Step 1855000, loss: -0.5593750987930107\n",
      "Step 1856000, loss: -0.46973527458193715\n",
      "Step 1857000, loss: -0.605323220759863\n",
      "Step 1858000, loss: -0.6602469810629263\n",
      "Step 1859000, loss: -0.5362909470184241\n",
      "Step 1860000, loss: -0.640888725045137\n",
      "Step 1861000, loss: -0.4444118933487916\n",
      "Step 1862000, loss: -0.5954458003360779\n",
      "Step 1863000, loss: -0.51533582662775\n",
      "Step 1864000, loss: -0.6246077347127285\n",
      "Step 1865000, loss: -0.6626092815455049\n",
      "Step 1866000, loss: -0.5899118840109441\n",
      "Step 1867000, loss: -0.46010445421561597\n",
      "Step 1868000, loss: -0.692867692031432\n",
      "Step 1869000, loss: -0.643582503608428\n",
      "Step 1870000, loss: -0.5110311344254296\n",
      "Step 1871000, loss: -0.5197318984140292\n",
      "Step 1872000, loss: -0.6546725542765344\n",
      "Step 1873000, loss: -0.5592681126451352\n",
      "Step 1874000, loss: -0.6704516901390162\n",
      "Step 1875000, loss: -0.6210106166433543\n",
      "Step 1876000, loss: -0.5332140316131408\n",
      "Step 1877000, loss: -0.6052880715101492\n",
      "Step 1878000, loss: -0.612330824753968\n",
      "Step 1879000, loss: -0.5879133187479456\n",
      "Step 1880000, loss: -0.687558802482672\n",
      "Step 1881000, loss: -0.5900060188600328\n",
      "Step 1882000, loss: -0.5363827653685584\n",
      "Step 1883000, loss: -0.6639144339985215\n",
      "Step 1884000, loss: -0.5917921430026181\n",
      "Step 1885000, loss: -0.6506330450004898\n",
      "Step 1886000, loss: -0.39253621126455257\n",
      "Step 1887000, loss: -0.5772794407422188\n",
      "Step 1888000, loss: -0.6470353473913856\n",
      "Step 1889000, loss: -0.5403351876439992\n",
      "Step 1890000, loss: -0.7710545777548105\n",
      "Step 1891000, loss: -0.34262985926680267\n",
      "Step 1892000, loss: -0.6200838306550868\n",
      "Step 1893000, loss: -0.7321352892210707\n",
      "Step 1894000, loss: -0.6703443253133446\n",
      "Step 1895000, loss: -0.2666165566574782\n",
      "Step 1896000, loss: -0.6075027472595684\n",
      "Step 1897000, loss: -0.6521533556880895\n",
      "Step 1898000, loss: -0.5965330635714344\n",
      "Step 1899000, loss: -0.5475927522989222\n",
      "Step 1900000, loss: -0.5872762104630237\n",
      "Step 1901000, loss: -0.5895364366583526\n",
      "Step 1902000, loss: -0.70654527807259\n",
      "Step 1903000, loss: -0.4977540116998716\n",
      "Step 1904000, loss: -0.7319191610834969\n",
      "Step 1905000, loss: -0.5208363645706849\n",
      "Step 1906000, loss: -0.6293438772326335\n",
      "Step 1907000, loss: -0.5152261509746313\n",
      "Step 1908000, loss: -0.6142486394434236\n",
      "Step 1909000, loss: -0.6411500538100954\n",
      "Step 1910000, loss: -0.6110818471883249\n",
      "Step 1911000, loss: -0.5988386682325654\n",
      "Step 1912000, loss: -0.6362808032375761\n",
      "Step 1913000, loss: -0.5968621596936137\n",
      "Step 1914000, loss: -0.6166756989579081\n",
      "Step 1915000, loss: -0.6536186159274076\n",
      "Step 1916000, loss: -0.5828633443699218\n",
      "Step 1917000, loss: -0.7065254943948239\n",
      "Step 1918000, loss: -0.5136473735996988\n",
      "Step 1919000, loss: -0.6606892050530295\n",
      "Step 1920000, loss: -0.5966847903986927\n",
      "Step 1921000, loss: -0.548665262210765\n",
      "Step 1922000, loss: -0.71856727386266\n",
      "Step 1923000, loss: -0.6223923985920846\n",
      "Step 1924000, loss: -0.623132868036977\n",
      "Step 1925000, loss: -0.6483524429649115\n",
      "Step 1926000, loss: -0.6363532779150409\n",
      "Step 1927000, loss: -0.4788233847511001\n",
      "Step 1928000, loss: -0.6386594403673226\n",
      "Step 1929000, loss: -0.6102618038584479\n",
      "Step 1930000, loss: -0.7314168175123632\n",
      "Step 1931000, loss: -0.6791167395071825\n",
      "Step 1932000, loss: -0.5002157535078005\n",
      "Step 1933000, loss: -0.5291914790878073\n",
      "Step 1934000, loss: -0.5936047994405963\n",
      "Step 1935000, loss: -0.6199875535335159\n",
      "Step 1936000, loss: -0.6196506968042813\n",
      "Step 1937000, loss: -0.7087402748814784\n",
      "Step 1938000, loss: -0.2786183721954003\n",
      "Step 1939000, loss: -0.7168994186182972\n",
      "Step 1940000, loss: -0.6728678137771785\n",
      "Step 1941000, loss: -0.6233213762012311\n",
      "Step 1942000, loss: -0.5982406431301497\n",
      "Step 1943000, loss: -0.6465966071959119\n",
      "Step 1944000, loss: -0.6128079722793773\n",
      "Step 1945000, loss: -0.6667268060257193\n",
      "Step 1946000, loss: -0.4177669801292941\n",
      "Step 1947000, loss: -0.7175936894482001\n",
      "Step 1948000, loss: -0.6506464520589798\n",
      "Step 1949000, loss: -0.6254955393252895\n",
      "Step 1950000, loss: -0.4578355096244486\n",
      "Step 1951000, loss: -0.7040034452674445\n",
      "Step 1952000, loss: -0.5450491376264254\n",
      "Step 1953000, loss: -0.6442985642040149\n",
      "Step 1954000, loss: -0.6749542050324381\n",
      "Step 1955000, loss: -0.4152510986337438\n",
      "Step 1956000, loss: -0.6182559856438311\n",
      "Step 1957000, loss: -0.633103210995905\n",
      "Step 1958000, loss: -0.6617716672606766\n",
      "Step 1959000, loss: -0.46604989748401565\n",
      "Step 1960000, loss: -0.7110312722399831\n",
      "Step 1961000, loss: -0.584158600086812\n",
      "Step 1962000, loss: -0.7168356618992985\n",
      "Step 1963000, loss: -0.6405973230423406\n",
      "Step 1964000, loss: -0.5768924724683165\n",
      "Step 1965000, loss: -0.5465590837097261\n",
      "Step 1966000, loss: -0.5226299871235387\n",
      "Step 1967000, loss: -0.7233293922198937\n",
      "Step 1968000, loss: -0.573040179870557\n",
      "Step 1969000, loss: -0.5967238645106263\n",
      "Step 1970000, loss: -0.6759749321392737\n",
      "Step 1971000, loss: -0.6872926588244737\n",
      "Step 1972000, loss: -0.5420646276610787\n",
      "Step 1973000, loss: -0.6559885280948947\n",
      "Step 1974000, loss: -0.5425360947370064\n",
      "Step 1975000, loss: -0.5705906030405313\n",
      "Step 1976000, loss: -0.6778278523674235\n",
      "Step 1977000, loss: -0.594806574913906\n",
      "Step 1978000, loss: -0.5401329788763541\n",
      "Step 1979000, loss: -0.7145869413744659\n",
      "Step 1980000, loss: -0.6828601838877657\n",
      "Step 1981000, loss: -0.6263264711478259\n",
      "Step 1982000, loss: -0.5480031991818687\n",
      "Step 1983000, loss: -0.7140905406172388\n",
      "Step 1984000, loss: -0.6094044263656251\n",
      "Step 1985000, loss: -0.7093149753718171\n",
      "Step 1986000, loss: -0.5463090524359141\n",
      "Step 1987000, loss: -0.7094125393470749\n",
      "Step 1988000, loss: -0.6641667645870475\n",
      "Step 1989000, loss: -0.6109359881398269\n",
      "Step 1990000, loss: -0.6959949746251368\n",
      "Step 1991000, loss: -0.6280061105801724\n",
      "Step 1992000, loss: -0.6700893430772121\n",
      "Step 1993000, loss: -0.6202714939193429\n",
      "Step 1994000, loss: -0.5680677983382557\n",
      "Step 1995000, loss: -0.6090952446076553\n",
      "Step 1996000, loss: -0.6469015113199712\n",
      "Step 1997000, loss: -0.6935750525835902\n",
      "Step 1998000, loss: -0.5888290926049804\n",
      "Step 1999000, loss: -0.7155315581052564\n"
     ]
    }
   ],
   "source": [
    "loss_checkpoint = 1000\n",
    "loss_inform_checkpoint = 1000\n",
    "plot_checkpoint = 10000\n",
    "\n",
    "avg_loss = 0.0\n",
    "for step in range(2000000):\n",
    "    x_t, y_t, x_m, y_m = dset.get_sample(16, max_context=5, max_target=10)\n",
    "    x_t = x_t.to(device)\n",
    "    y_t = y_t.to(device)\n",
    "    x_m = x_m.to(device)\n",
    "    y_m = y_m.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    #print(x_t.shape, y_t.shape, x_m.shape, y_m.shape)\n",
    "    loss = model.nll_loss(x_t, y_t[..., :1], y_t[..., 1:], x_m, y_m)\n",
    "    #if loss.item()< 1000:\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    avg_loss += loss.item()\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step {step}, loss: {avg_loss / 1000}\")\n",
    "        avg_loss = 0.0\n",
    "\n",
    "    if step % 10000 == 0:\n",
    "        torch.save(model.state_dict(), 'cnp_dance_3d_0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: -0.00017367644608020782\n",
      "Step 1000, loss: 0.1869754249593243\n",
      "Step 2000, loss: 0.04706582804769278\n",
      "Step 3000, loss: 0.12975088268530088\n",
      "Step 4000, loss: 0.07417581472445454\n",
      "Step 5000, loss: 0.18324905021110316\n",
      "Step 6000, loss: 0.12731562045024475\n",
      "Step 7000, loss: 0.4265879245174001\n",
      "Step 8000, loss: 0.1413722164109713\n",
      "Step 9000, loss: 0.35957774856322067\n",
      "Step 10000, loss: 0.06377134860644583\n",
      "Step 11000, loss: 0.03485903262568172\n",
      "Step 12000, loss: 0.3192748798822431\n",
      "Step 13000, loss: 0.09295735764270649\n",
      "Step 14000, loss: 0.5434923594161519\n",
      "Step 15000, loss: 0.42852468137885924\n",
      "Step 16000, loss: 0.14081505185572313\n",
      "Step 17000, loss: 0.12017125783368102\n",
      "Step 18000, loss: 0.26581044430856127\n",
      "Step 19000, loss: 0.11680626166274305\n",
      "Step 20000, loss: 0.5219039054016467\n",
      "Step 21000, loss: 0.10127220950671471\n",
      "Step 22000, loss: 0.3481752949623558\n",
      "Step 23000, loss: 0.19639951749003376\n",
      "Step 24000, loss: 0.32117426470537974\n",
      "Step 25000, loss: 0.22408723960071802\n",
      "Step 26000, loss: 0.3884058392938459\n",
      "Step 27000, loss: 0.12355196259950753\n",
      "Step 28000, loss: 0.03599014701620763\n",
      "Step 29000, loss: 0.1687117975825531\n",
      "Step 30000, loss: 0.5278889962835237\n",
      "Step 31000, loss: 0.05823334587179124\n",
      "Step 32000, loss: 0.16676992299442644\n",
      "Step 33000, loss: -0.02615772482031025\n",
      "Step 34000, loss: 0.7146781387600714\n",
      "Step 35000, loss: 0.06349273931840435\n",
      "Step 36000, loss: 0.11211368842855882\n",
      "Step 37000, loss: 0.07334700884902849\n",
      "Step 38000, loss: 0.2922139630758029\n",
      "Step 39000, loss: -0.016101568763784598\n",
      "Step 40000, loss: 0.291852599263133\n",
      "Step 41000, loss: 0.06052996140989853\n",
      "Step 42000, loss: 0.2158121031167684\n",
      "Step 43000, loss: 0.21682346815237544\n",
      "Step 44000, loss: 0.025519280176609753\n",
      "Step 45000, loss: 0.365888875246339\n",
      "Step 46000, loss: 0.1647191867606016\n",
      "Step 47000, loss: 0.2667236497954\n",
      "Step 48000, loss: 0.18679473369650076\n",
      "Step 49000, loss: 0.1448777407555608\n",
      "Step 50000, loss: 0.6528571227727225\n",
      "Step 51000, loss: 0.14906250188767445\n",
      "Step 52000, loss: 0.08204986642919539\n",
      "Step 53000, loss: 0.2637347315679945\n",
      "Step 54000, loss: 0.17252952835330507\n",
      "Step 55000, loss: 0.03180766625518845\n",
      "Step 56000, loss: 0.5717094865397084\n",
      "Step 57000, loss: 0.10805747803088162\n",
      "Step 58000, loss: 0.4307661495140637\n",
      "Step 59000, loss: 0.1227388327098015\n",
      "Step 60000, loss: 0.12744078077253654\n",
      "Step 61000, loss: 0.40521867024051605\n",
      "Step 62000, loss: 0.07140267651578688\n",
      "Step 63000, loss: 0.2699676200687536\n",
      "Step 64000, loss: 0.07838408072025049\n",
      "Step 65000, loss: 0.29917463549558304\n",
      "Step 66000, loss: 0.573214099280638\n",
      "Step 67000, loss: 0.1238085525665665\n",
      "Step 68000, loss: 0.2896953802529315\n",
      "Step 69000, loss: 0.14014895963305024\n",
      "Step 70000, loss: -0.12045683638122864\n",
      "Step 71000, loss: 0.11512164079095237\n",
      "Step 72000, loss: 0.11957167871946876\n",
      "Step 73000, loss: 0.3839406085505034\n",
      "Step 74000, loss: 0.24808461639494636\n",
      "Step 75000, loss: 0.30219061850826257\n",
      "Step 76000, loss: 0.22972862923907814\n",
      "Step 77000, loss: 0.15276511317209224\n",
      "Step 78000, loss: 0.1512437256319099\n",
      "Step 79000, loss: 0.08411908983346075\n",
      "Step 80000, loss: -0.0798552372896811\n",
      "Step 81000, loss: 0.10721864595693478\n",
      "Step 82000, loss: 0.15026527336379514\n",
      "Step 83000, loss: 0.14600926842492662\n",
      "Step 84000, loss: -0.07567025169129192\n",
      "Step 85000, loss: 0.13185183444595897\n",
      "Step 86000, loss: 0.46199787052709146\n",
      "Step 87000, loss: 0.2944458604855463\n",
      "Step 88000, loss: 0.0541710253074998\n",
      "Step 89000, loss: 0.13896923360304209\n",
      "Step 90000, loss: 0.24128264148075323\n",
      "Step 91000, loss: 0.3082456262192573\n",
      "Step 92000, loss: -0.00560605964253773\n",
      "Step 93000, loss: 0.23602597735484596\n",
      "Step 94000, loss: 0.47132756964437433\n",
      "Step 95000, loss: -0.09538883338881715\n",
      "Step 96000, loss: -0.024584354452439585\n",
      "Step 97000, loss: -0.08272915064234986\n",
      "Step 98000, loss: 0.13355533491482494\n",
      "Step 99000, loss: 0.17490609829702225\n",
      "Step 100000, loss: 0.14791991439776028\n",
      "Step 101000, loss: 0.017925902518240035\n",
      "Step 102000, loss: 0.36051664550663554\n",
      "Step 103000, loss: 0.21327952979994005\n",
      "Step 104000, loss: 0.028544456884819738\n",
      "Step 105000, loss: 0.08024423422676046\n",
      "Step 106000, loss: 0.28159014735303933\n",
      "Step 107000, loss: -0.02142823042918462\n",
      "Step 108000, loss: 0.1662302737053251\n",
      "Step 109000, loss: 0.21954757451976184\n",
      "Step 110000, loss: 0.37635379006364383\n",
      "Step 111000, loss: 0.003201612417164142\n",
      "Step 112000, loss: 0.11942772260517813\n",
      "Step 113000, loss: -0.011005606446444289\n",
      "Step 114000, loss: -0.029721548159912344\n",
      "Step 115000, loss: 0.4540843798957649\n",
      "Step 116000, loss: -0.0012813820857845713\n",
      "Step 117000, loss: 0.008207498647272587\n",
      "Step 118000, loss: 0.2883929672606755\n",
      "Step 119000, loss: 0.0985487262335082\n",
      "Step 120000, loss: 0.060451565892268265\n",
      "Step 121000, loss: 0.21295827004489185\n",
      "Step 122000, loss: 0.06985980685526738\n",
      "Step 123000, loss: 0.018768423041561618\n",
      "Step 124000, loss: 0.4000201143162558\n",
      "Step 125000, loss: 0.15390907321273697\n",
      "Step 126000, loss: 0.09426795695268084\n",
      "Step 127000, loss: 0.3582521294495236\n",
      "Step 128000, loss: -0.13507080338598462\n",
      "Step 129000, loss: 0.2965940339860972\n",
      "Step 130000, loss: 0.039497716086625584\n",
      "Step 131000, loss: 0.08673476455637137\n",
      "Step 132000, loss: -0.01976472111392286\n",
      "Step 133000, loss: 0.11466540327918483\n",
      "Step 134000, loss: 0.24443910440886793\n",
      "Step 135000, loss: 0.17281284213575418\n",
      "Step 136000, loss: -0.022208595251227963\n",
      "Step 137000, loss: 0.5005425782795937\n",
      "Step 138000, loss: -0.07418625407126092\n",
      "Step 139000, loss: 0.13479211991105694\n",
      "Step 140000, loss: 0.1595520181322936\n",
      "Step 141000, loss: 0.19591880122385918\n",
      "Step 142000, loss: -0.007329898622701876\n",
      "Step 143000, loss: -0.06055261651513138\n",
      "Step 144000, loss: 0.18817083432537038\n",
      "Step 145000, loss: 0.0578731118102005\n",
      "Step 146000, loss: 0.13357359325163998\n",
      "Step 147000, loss: -0.026032329622306862\n",
      "Step 148000, loss: 0.48503571660479067\n",
      "Step 149000, loss: 0.12466232052393025\n",
      "Step 150000, loss: 0.4973502241774113\n",
      "Step 151000, loss: 0.08398948599421419\n",
      "Step 152000, loss: -0.049816610282636245\n",
      "Step 153000, loss: 0.1630063247201906\n",
      "Step 154000, loss: 0.16333014741165244\n",
      "Step 155000, loss: 0.3990647457530285\n",
      "Step 156000, loss: 0.41338194962922353\n",
      "Step 157000, loss: 0.39066968680915304\n",
      "Step 158000, loss: 0.2306313449039153\n",
      "Step 159000, loss: 0.04569915710552595\n",
      "Step 160000, loss: 0.10566658510221169\n",
      "Step 161000, loss: 0.2731138055040501\n",
      "Step 162000, loss: 0.021526075396162924\n",
      "Step 163000, loss: 0.23275654214932\n",
      "Step 164000, loss: 0.3612021792812884\n",
      "Step 165000, loss: 0.20984474132501055\n",
      "Step 166000, loss: 0.19144432654581034\n",
      "Step 167000, loss: 0.06556382361927536\n",
      "Step 168000, loss: 0.6145752244335599\n",
      "Step 169000, loss: 0.03881805762927979\n",
      "Step 170000, loss: 0.19090248816180974\n",
      "Step 171000, loss: 0.06211277513881214\n",
      "Step 172000, loss: -0.11610525411251\n",
      "Step 173000, loss: -0.030134133415907854\n",
      "Step 174000, loss: 0.1483407946093066\n",
      "Step 175000, loss: 0.272821362344228\n",
      "Step 176000, loss: 0.13512784005433787\n",
      "Step 177000, loss: 0.05370523227099329\n",
      "Step 178000, loss: 0.3950947466546204\n",
      "Step 179000, loss: 0.005003427292802371\n",
      "Step 180000, loss: -0.03043710671691224\n",
      "Step 181000, loss: -0.019700049953273263\n",
      "Step 182000, loss: 0.06171245051990263\n",
      "Step 183000, loss: 0.24159596514096485\n",
      "Step 184000, loss: 0.025083111130836187\n",
      "Step 185000, loss: 0.08625506329751806\n",
      "Step 186000, loss: 0.5278052255921066\n",
      "Step 187000, loss: 0.29985670068778564\n",
      "Step 188000, loss: 0.17781163619176368\n",
      "Step 189000, loss: 0.6445462882114807\n",
      "Step 190000, loss: 0.10818451813259163\n",
      "Step 191000, loss: 0.2979162965175929\n",
      "Step 192000, loss: 0.029883597612846643\n",
      "Step 193000, loss: 0.039695637631812136\n",
      "Step 194000, loss: -0.05629612971795723\n",
      "Step 195000, loss: -0.08393386017996818\n",
      "Step 196000, loss: 0.17267192878900095\n",
      "Step 197000, loss: 0.40275396961451043\n",
      "Step 198000, loss: 0.17212064047079184\n",
      "Step 199000, loss: 0.1575477802644018\n",
      "Step 200000, loss: 0.16165438504342455\n",
      "Step 201000, loss: 0.04676220899040345\n",
      "Step 202000, loss: -0.034293422846356406\n",
      "Step 203000, loss: 0.14007158070075093\n",
      "Step 204000, loss: -0.023275139004777883\n",
      "Step 205000, loss: 0.2739890044298809\n",
      "Step 206000, loss: 0.05019318275369005\n",
      "Step 207000, loss: 0.020013050865440165\n",
      "Step 208000, loss: -0.04245633580861613\n",
      "Step 209000, loss: 0.2796160166241316\n",
      "Step 210000, loss: -0.021895163375651464\n",
      "Step 211000, loss: 0.0847311401439074\n",
      "Step 212000, loss: -0.07584732033940964\n",
      "Step 213000, loss: 0.34245412709866285\n",
      "Step 214000, loss: 0.0179673457002109\n",
      "Step 215000, loss: 0.28667567300860536\n",
      "Step 216000, loss: 0.1988590255487652\n",
      "Step 217000, loss: 0.1323873221912072\n",
      "Step 218000, loss: 0.33860665876550045\n",
      "Step 219000, loss: 0.034225228375231384\n",
      "Step 220000, loss: -0.002314856478536967\n",
      "Step 221000, loss: 0.27856271673884475\n",
      "Step 222000, loss: -0.03754483640450053\n",
      "Step 223000, loss: 0.2810787939079455\n",
      "Step 224000, loss: 0.05529613387565769\n",
      "Step 225000, loss: -0.09468695090524852\n",
      "Step 226000, loss: 0.3120209763637686\n",
      "Step 227000, loss: 0.3424389351780992\n",
      "Step 228000, loss: 0.3459527246663056\n",
      "Step 229000, loss: 0.6084945940705948\n",
      "Step 230000, loss: 0.24239655058970674\n",
      "Step 231000, loss: -0.00702370203146711\n",
      "Step 232000, loss: 0.08654569717674167\n",
      "Step 233000, loss: -0.026446501694386827\n",
      "Step 234000, loss: 0.28502502828798604\n",
      "Step 235000, loss: 0.08863116401212756\n",
      "Step 236000, loss: 0.041195556282982576\n",
      "Step 237000, loss: 0.09410094234562712\n",
      "Step 238000, loss: 0.05738279958604835\n",
      "Step 239000, loss: 0.03853602435765788\n",
      "Step 240000, loss: -0.2005922336838521\n",
      "Step 241000, loss: 0.3999692843898665\n",
      "Step 242000, loss: 0.2497645600536489\n",
      "Step 243000, loss: -0.0971187531343312\n",
      "Step 244000, loss: -0.09970865856064484\n",
      "Step 245000, loss: 0.1239416374625871\n",
      "Step 246000, loss: 0.05720445646838198\n",
      "Step 247000, loss: 0.2932097611527497\n",
      "Step 248000, loss: 0.5130843510660197\n",
      "Step 249000, loss: 0.015531985578498279\n",
      "Step 250000, loss: -0.14092835141834803\n",
      "Step 251000, loss: -0.0003350054835318588\n",
      "Step 252000, loss: 0.22695121704082702\n",
      "Step 253000, loss: -0.04569518233297276\n",
      "Step 254000, loss: 0.4231661362281302\n",
      "Step 255000, loss: 0.06362210620760016\n",
      "Step 256000, loss: -0.07335137644386851\n",
      "Step 257000, loss: 0.08517445761352428\n",
      "Step 258000, loss: 0.24775208540115273\n",
      "Step 259000, loss: 0.1514996394849659\n",
      "Step 260000, loss: 0.08300134237180463\n",
      "Step 261000, loss: 0.33455882412794746\n",
      "Step 262000, loss: 0.03409828433697112\n",
      "Step 263000, loss: 0.2723094663068186\n",
      "Step 264000, loss: 0.03359235753957182\n",
      "Step 265000, loss: 0.4195555991764122\n",
      "Step 266000, loss: 0.25627856913255526\n",
      "Step 267000, loss: 0.3391276911939494\n",
      "Step 268000, loss: 0.14737688840157354\n",
      "Step 269000, loss: -0.052277568578603675\n",
      "Step 270000, loss: -0.0845370653726277\n",
      "Step 271000, loss: -0.04858067692903569\n",
      "Step 272000, loss: 0.36017050053178173\n",
      "Step 273000, loss: 0.00333318518383021\n",
      "Step 274000, loss: 0.07710396187281003\n",
      "Step 275000, loss: -0.1850153931863606\n",
      "Step 276000, loss: 0.35672526144596484\n",
      "Step 277000, loss: 0.11188840370455426\n",
      "Step 278000, loss: 0.21321915772289504\n",
      "Step 279000, loss: 0.03719890008967923\n",
      "Step 280000, loss: 0.09155523468344472\n",
      "Step 281000, loss: 0.2598657270465628\n",
      "Step 282000, loss: 0.001348402195784729\n",
      "Step 283000, loss: 0.20398970470199856\n",
      "Step 284000, loss: 0.17572464471002605\n",
      "Step 285000, loss: 0.24654718851897633\n",
      "Step 286000, loss: 0.026179437903803775\n",
      "Step 287000, loss: 0.19429953064555594\n",
      "Step 288000, loss: -0.051021047587506474\n",
      "Step 289000, loss: 0.41457582824199923\n",
      "Step 290000, loss: 0.10541972452815389\n",
      "Step 291000, loss: 0.1446143900700554\n",
      "Step 292000, loss: 0.3731656315936707\n",
      "Step 293000, loss: -0.12513221878209152\n",
      "Step 294000, loss: -0.03596511284378357\n",
      "Step 295000, loss: 0.03047814844687673\n",
      "Step 296000, loss: 0.053587515619587064\n",
      "Step 297000, loss: 0.2622010333834187\n",
      "Step 298000, loss: 0.2915983478183625\n",
      "Step 299000, loss: -0.0018779335269646254\n",
      "Step 300000, loss: 0.5255523684340005\n",
      "Step 301000, loss: 0.014232078518922208\n",
      "Step 302000, loss: 0.07646374128958268\n",
      "Step 303000, loss: -0.12982336479087825\n",
      "Step 304000, loss: 0.4481616099605326\n",
      "Step 305000, loss: 0.15279013167723315\n",
      "Step 306000, loss: 0.09239244775488624\n",
      "Step 307000, loss: 0.24009675308756415\n",
      "Step 308000, loss: 0.05383363440982066\n",
      "Step 309000, loss: -0.09767801076802425\n",
      "Step 310000, loss: 0.14898364653595492\n",
      "Step 311000, loss: 0.42357970668771305\n",
      "Step 312000, loss: 0.016113957130874042\n",
      "Step 313000, loss: -0.004014315565931611\n",
      "Step 314000, loss: -0.05800450527714565\n",
      "Step 315000, loss: 0.8142632895074785\n",
      "Step 316000, loss: -0.1949594831965951\n",
      "Step 317000, loss: -0.22299033254978712\n",
      "Step 318000, loss: 0.11662670653630039\n",
      "Step 319000, loss: -0.015892355978139677\n",
      "Step 320000, loss: 0.009179560696018598\n",
      "Step 321000, loss: 0.10043188796215691\n",
      "Step 322000, loss: 0.5793960021176027\n",
      "Step 323000, loss: 0.16043444215226918\n",
      "Step 324000, loss: 0.13844023211835885\n",
      "Step 325000, loss: 0.33454141919250835\n",
      "Step 326000, loss: 0.16368543396623864\n",
      "Step 327000, loss: 0.0987498126585706\n",
      "Step 328000, loss: 0.18331058376924192\n",
      "Step 329000, loss: -0.07015631001675501\n",
      "Step 330000, loss: 0.02488266895448032\n",
      "Step 331000, loss: 0.2200430105549749\n",
      "Step 332000, loss: -0.12952796725187363\n",
      "Step 333000, loss: 0.2483475458852481\n",
      "Step 334000, loss: 0.015524770413525403\n",
      "Step 335000, loss: -0.09791997716771585\n",
      "Step 336000, loss: -0.04172690729470924\n",
      "Step 337000, loss: 0.09671232416376006\n",
      "Step 338000, loss: 0.438096937475806\n",
      "Step 339000, loss: -0.1616427132145036\n",
      "Step 340000, loss: 0.06105986898206174\n",
      "Step 341000, loss: 0.0379857462157961\n",
      "Step 342000, loss: -0.06788114363560453\n",
      "Step 343000, loss: -0.06468333880777936\n",
      "Step 344000, loss: 0.19984202139172702\n",
      "Step 345000, loss: 0.006686913591751363\n",
      "Step 346000, loss: -0.05732218451306108\n",
      "Step 347000, loss: -0.03541232130181743\n",
      "Step 348000, loss: 0.09257902274555818\n",
      "Step 349000, loss: 0.2866782723990036\n",
      "Step 350000, loss: 0.1327616380203981\n",
      "Step 351000, loss: -0.03001021426913212\n",
      "Step 352000, loss: 0.2898378759930565\n",
      "Step 353000, loss: 0.06738725216279272\n",
      "Step 354000, loss: 0.18257585287176334\n",
      "Step 355000, loss: -0.08905363771365955\n",
      "Step 356000, loss: 0.0669337845883565\n",
      "Step 357000, loss: -0.07422913535538828\n",
      "Step 358000, loss: -0.06009447124818689\n",
      "Step 359000, loss: -0.26705976186145564\n",
      "Step 360000, loss: 0.017589872980490327\n",
      "Step 361000, loss: 0.05936965656885877\n",
      "Step 362000, loss: -0.12995206446752128\n",
      "Step 363000, loss: 0.02280062319210265\n",
      "Step 364000, loss: -0.09661416181553796\n",
      "Step 365000, loss: -0.1280737614510872\n",
      "Step 366000, loss: -0.0023950980648514814\n",
      "Step 367000, loss: -0.10682692589814542\n",
      "Step 368000, loss: 0.00604026862988394\n",
      "Step 369000, loss: 0.20039901699393523\n",
      "Step 370000, loss: -0.09796331702634416\n",
      "Step 371000, loss: 0.29717045306644285\n",
      "Step 372000, loss: 0.09642863925988786\n",
      "Step 373000, loss: -0.09293021641575615\n",
      "Step 374000, loss: -0.10679199886361311\n",
      "Step 375000, loss: -0.24960165257248446\n",
      "Step 376000, loss: 0.1599341403093422\n",
      "Step 377000, loss: -0.12526665634225356\n",
      "Step 378000, loss: 0.26871057830564676\n",
      "Step 379000, loss: -0.3108244170139951\n",
      "Step 380000, loss: 0.047932398109871426\n",
      "Step 381000, loss: -0.048346801731386224\n",
      "Step 382000, loss: 0.11700735589233227\n",
      "Step 383000, loss: -0.13891427239903714\n",
      "Step 384000, loss: 0.3663679063578602\n",
      "Step 385000, loss: -0.1712109738563595\n",
      "Step 386000, loss: 0.06989966160814219\n",
      "Step 387000, loss: -0.11571773466913146\n",
      "Step 388000, loss: -0.03533331544836983\n",
      "Step 389000, loss: -0.29431144967308503\n",
      "Step 390000, loss: 0.16021251981065143\n",
      "Step 391000, loss: -0.22480551556509454\n",
      "Step 392000, loss: 0.3166811582938099\n",
      "Step 393000, loss: -0.15706805927178358\n",
      "Step 394000, loss: -0.042137615241575985\n",
      "Step 395000, loss: -0.11010990017931908\n",
      "Step 396000, loss: 0.029594934374821604\n",
      "Step 397000, loss: -0.05548372545000166\n",
      "Step 398000, loss: -0.023757076998743288\n",
      "Step 399000, loss: -0.12361511957924813\n",
      "Step 400000, loss: -0.3143364310411271\n",
      "Step 401000, loss: 0.0036294098558137195\n",
      "Step 402000, loss: -0.09039359467287432\n",
      "Step 403000, loss: 0.08436875972128473\n",
      "Step 404000, loss: 0.014632984376279638\n",
      "Step 405000, loss: -0.20543224129633744\n",
      "Step 406000, loss: -0.037015354719827885\n",
      "Step 407000, loss: -0.028498622264048663\n",
      "Step 408000, loss: -0.32929821079736576\n",
      "Step 409000, loss: 0.12923440472525544\n",
      "Step 410000, loss: -0.18787360957660712\n",
      "Step 411000, loss: -0.22434060819039586\n",
      "Step 412000, loss: -0.14426544083254703\n",
      "Step 413000, loss: 0.1082376792820869\n",
      "Step 414000, loss: -0.3694285041928524\n",
      "Step 415000, loss: -0.1297993066154304\n",
      "Step 416000, loss: -0.11892461424670182\n",
      "Step 417000, loss: -0.0065054760487983\n",
      "Step 418000, loss: -0.20522529325605138\n",
      "Step 419000, loss: -0.31433568823710084\n",
      "Step 420000, loss: 0.27484279736043393\n",
      "Step 421000, loss: -0.12943250122154132\n",
      "Step 422000, loss: -0.12108472558890936\n",
      "Step 423000, loss: -0.14254617718409282\n",
      "Step 424000, loss: 0.16811968895874452\n",
      "Step 425000, loss: -0.08678117947262945\n",
      "Step 426000, loss: 0.26856352249588056\n",
      "Step 427000, loss: -0.23414211926353165\n",
      "Step 428000, loss: -0.0956835400161217\n",
      "Step 429000, loss: -0.09427875380078331\n",
      "Step 430000, loss: -0.018909022993291728\n",
      "Step 431000, loss: -0.007115685620694421\n",
      "Step 432000, loss: -0.1422532321223989\n",
      "Step 433000, loss: 0.05114697273040656\n",
      "Step 434000, loss: -0.31263498023455033\n",
      "Step 435000, loss: -0.16802064739930211\n",
      "Step 436000, loss: -0.3253090674944106\n",
      "Step 437000, loss: 0.07921822778484784\n",
      "Step 438000, loss: -0.2616403816047241\n",
      "Step 439000, loss: -0.09794921360490844\n",
      "Step 440000, loss: 0.010767446798563469\n",
      "Step 441000, loss: -0.09115045618050499\n",
      "Step 442000, loss: -0.11258893682327471\n",
      "Step 443000, loss: -0.13843474044196774\n",
      "Step 444000, loss: 0.12923579810140653\n",
      "Step 445000, loss: 0.13165688773128204\n",
      "Step 446000, loss: -0.13767023141578466\n",
      "Step 447000, loss: -0.07176031129742842\n",
      "Step 448000, loss: -0.22967686023380612\n",
      "Step 449000, loss: 0.06483139195095282\n",
      "Step 450000, loss: -0.2512038874475365\n",
      "Step 451000, loss: 0.23966878602886574\n",
      "Step 452000, loss: -0.2308518844906357\n",
      "Step 453000, loss: -0.13026000298907184\n",
      "Step 454000, loss: -0.165844946459285\n",
      "Step 455000, loss: -0.07913750784442528\n",
      "Step 456000, loss: 0.015019382711267099\n",
      "Step 457000, loss: 0.06863795096706599\n",
      "Step 458000, loss: -0.35871726236736867\n",
      "Step 459000, loss: -0.23939537656668108\n",
      "Step 460000, loss: 0.16777577492540877\n",
      "Step 461000, loss: -0.3321030952415895\n",
      "Step 462000, loss: -0.1801579348947853\n",
      "Step 463000, loss: -0.122354342646664\n",
      "Step 464000, loss: -0.30914018439967184\n",
      "Step 465000, loss: 0.22623475846508517\n",
      "Step 466000, loss: -0.232786623713444\n",
      "Step 467000, loss: -0.1438221930573345\n",
      "Step 468000, loss: -0.038923241950629746\n",
      "Step 469000, loss: 0.02716526935430011\n",
      "Step 470000, loss: -0.15284378431797813\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/colors/aist_lw/aistplusplus_api-main/skeleton.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colors/aist_lw/aistplusplus_api-main/skeleton.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#if loss.item()< 1000:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colors/aist_lw/aistplusplus_api-main/skeleton.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/colors/aist_lw/aistplusplus_api-main/skeleton.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer_bad\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colors/aist_lw/aistplusplus_api-main/skeleton.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colors/aist_lw/aistplusplus_api-main/skeleton.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_checkpoint = 1000\n",
    "loss_inform_checkpoint = 1000\n",
    "plot_checkpoint = 10000\n",
    "\n",
    "avg_loss = 0.0\n",
    "for step in range(2000000):\n",
    "    x_t, y_t, x_m, y_m = dset.get_sample(16, max_context=5, max_target=10)\n",
    "    x_t = x_t.to(device)\n",
    "    y_t = y_t.to(device)\n",
    "    x_m = x_m.to(device)\n",
    "    y_m = y_m.to(device)\n",
    "    optimizer_bad.zero_grad()\n",
    "    #print(x_t.shape, y_t.shape, x_m.shape, y_m.shape)\n",
    "    loss = model_bad.nll_loss(x_t, y_t[..., :1], y_t[..., 1:], x_m, y_m)\n",
    "    #if loss.item()< 1000:\n",
    "    loss.backward()\n",
    "    optimizer_bad.step()\n",
    "    avg_loss += loss.item()\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step {step}, loss: {avg_loss / 1000}\")\n",
    "        avg_loss = 0.0\n",
    "\n",
    "    if step % 10000 == 0:\n",
    "        torch.save(model_bad.state_dict(), 'cnp_dance_0_6_bad.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/colors/aist_lw/aistplusplus_api-main/skeleton.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colors/aist_lw/aistplusplus_api-main/skeleton.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m std\u001b[39m.\u001b[39msqueeze_(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colors/aist_lw/aistplusplus_api-main/skeleton.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Now we plot the predictions\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colors/aist_lw/aistplusplus_api-main/skeleton.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# We assume that 'trainset.data' contains the actual data and is compatible with the predictions\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/colors/aist_lw/aistplusplus_api-main/skeleton.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m trainset\u001b[39m.\u001b[39mdata:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colors/aist_lw/aistplusplus_api-main/skeleton.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# Plot the actual data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colors/aist_lw/aistplusplus_api-main/skeleton.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     plt\u001b[39m.\u001b[39mplot(d[:, \u001b[39m0\u001b[39m], d[:, \u001b[39m1\u001b[39m:], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mActual Data\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Adjust indices based on your actual data format\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/colors/aist_lw/aistplusplus_api-main/skeleton.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Plot the model's mean predictions; adjust indices if your data has a different format\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainset' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'observation' is already defined and on the correct device\n",
    "# 'target' tensor is defined as in your snippet and moved to the correct device\n",
    "target = torch.linspace(0.0, 1.0, 200).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    mean, std = model(observation, target, locally_weighted=True, aggregation_std=0.5)\n",
    "    # If you have other variations of the model call, you can use them as needed.\n",
    "\n",
    "# Squeeze the unnecessary dimensions if your model outputs extra batch dimensions\n",
    "mean.squeeze_(0)\n",
    "std.squeeze_(0)\n",
    "\n",
    "# Now we plot the predictions\n",
    "# We assume that 'trainset.data' contains the actual data and is compatible with the predictions\n",
    "for d in trainset.data:\n",
    "    # Plot the actual data\n",
    "    plt.plot(d[:, 0], d[:, 1:], label='Actual Data')  # Adjust indices based on your actual data format\n",
    "\n",
    "# Plot the model's mean predictions; adjust indices if your data has a different format\n",
    "plt.plot(target.cpu().numpy(), mean.cpu().numpy(), label='Model Predictions', color='red')\n",
    "\n",
    "# Optionally, you can fill between mean ± std to show the uncertainty\n",
    "plt.fill_between(target.cpu().numpy().flatten(),\n",
    "                 (mean - std).cpu().numpy().flatten(),\n",
    "                 (mean + std).cpu().numpy().flatten(),\n",
    "                 color='red', alpha=0.2, label='Prediction Uncertainty')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cnp_dance_3d_0.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
